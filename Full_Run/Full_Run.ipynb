{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "handed-mixture",
   "metadata": {},
   "source": [
    "To see how the initial simulation was run and the initial RDF calcaulted see /Setup/Na-Na_Setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-strip",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "intellectual-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MagicTools as mt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import os\n",
    "\n",
    "import string\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import random\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-winner",
   "metadata": {},
   "source": [
    "## Running the LAMMPS simulations\n",
    "\n",
    "It is necessary to run a large number of LAMMPS simulations to generate the training data for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "proprietary-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rdf = mt.ReadRDF('Setup/NaNa.rdf', quiet= True)\n",
    "\n",
    "Min = 0  \n",
    "Max = input_rdf.DFs[0].Max #range of r for potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "portable-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbid = input_rdf.DFs[0].x[0] #this is the starting point for any potential, below this potential should be large\n",
    "#this should be altered so that if the RDF contains very low values at the start (say <= 0.1), these likely still are\n",
    "#part of the forbidden region and should be treated as such\n",
    "\n",
    "\n",
    "stepsize = input_rdf.DFs[0].x[1] - input_rdf.DFs[0].x[0] #the distance between \n",
    "rvalues = np.arange(Min + stepsize/2, Max + stepsize/2, stepsize) #an array containg the r values\n",
    "\n",
    "for i in range(len(rvalues)):\n",
    "    rvalues[i] = round(rvalues[i], 6) #rounding the rvalues to prevent floating point errors\n",
    "    \n",
    "NPoints = len(rvalues) #the neumber of points in each RDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-poland",
   "metadata": {},
   "source": [
    "Need to create the trial potentials for the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phantom-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_potential(N, forbidden): \n",
    "    #this function creates a random potential of N points\n",
    "    #forbidden region size forbid (in aangstroms)\n",
    "    #the potential is smoothed using a savgol filter\n",
    "    \n",
    "    pot = np.zeros(N) #the array which will hold the potential\n",
    "    \n",
    "    rs = list(rvalues)\n",
    "    forbid = rs.index(forbidden) #determines the index of the forbidden region (where in the list of rvalues it appears)\n",
    "    \n",
    "    for i in range(forbid):\n",
    "        pot[i] = 75000 - 50*i \n",
    "    #within the forbidden region the potential is simply a very large potential, decreasing linearly\n",
    "\n",
    "    non_forbid = np.zeros(N - forbid) #non_forbid contains the potenital outside the forbidden region\n",
    "    non_forbid[0] = random.gauss(0, 2.5) #The value of the potential just outside the forbidden region is \n",
    "    #taken from a Gaussian centred on zero with standard deviation 2.5\n",
    "    \n",
    "    i = 1\n",
    "    \n",
    "    while i < len(non_forbid):\n",
    "        non_forbid[i] = non_forbid[i - 1] + random.random() * random.gauss(0, .5)\n",
    "        #this determines how the potential at each point is calculated based on the previous point\n",
    "        #currently each point is equal to the previous point + value where\n",
    "        #value is taken from a gaussian centred at 0 with SD 0.5, which is then multiplied by a random weight between\n",
    "        #0 and 1\n",
    "        i = i + 1\n",
    "\n",
    "    non_forbid_smooth = savgol_filter(non_forbid, 31, 3)\n",
    "    #smooths the potential outside the forbidden region using a savgol filter\n",
    "    \n",
    "    while max(non_forbid_smooth) - min(non_forbid_smooth) > 15:\n",
    "        non_forbid_smooth = 0.9*non_forbid_smooth\n",
    "    #if the potential covers too large a range, multiply every entry by a factor of 0.9\n",
    "    \n",
    "    for i in range(len(non_forbid_smooth)):\n",
    "        pot[forbid + i] = non_forbid_smooth[i]\n",
    "    #attach the forbidden and non-forbidden regions to create the full potential\n",
    "\n",
    "    shift = pot[len(pot) - 1]\n",
    "    \n",
    "    pot = pot - shift\n",
    "    #shift the entire potenital so the potenital goes to zero at the cut off distance\n",
    "    \n",
    "    return pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "german-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "potentials = []\n",
    "for i in range(10000):\n",
    "    trialpot = create_potential(len(rvalues), forbid)\n",
    "    potentials.append(trialpot)\n",
    "    \n",
    "#creates 10,000 potentials in the list potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-recording",
   "metadata": {},
   "source": [
    "Writing the potentials to MagiC then to LAMMPS potential files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daily-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_potential(filename, k):\n",
    "    #write potentials is a function which writes a magic potential file\n",
    "    #the first argument is the filename, output will be <filename>.pot\n",
    "    #k is the index of potential used\n",
    "    \n",
    "    N = len(input_rdf.AtomTypes)\n",
    "    N_NB = int(N*(N+1)/2) #number of non-bonded interactions\n",
    "    N_B = int(0) #number of bonded interactions\n",
    "    N_A = int(0) #number of angle interactions\n",
    "\n",
    "    AtomList = []\n",
    "    for i in range(len(input_rdf.AtomTypes)):\n",
    "        AtomList.append(input_rdf.AtomTypes[i])\n",
    "        \n",
    "    AtomPairs = list(it.combinations_with_replacement(AtomList, r = 2)) #all possible atoms pairs, no repeats\n",
    "    \n",
    "    general_section = \" &General \\n\" + \" NTypes= {} \\n\".format(N) + \" N_NB= {} \\n\".format(N_NB) + \\\n",
    "    \" N_B= {} \\n\".format(N_B) + \" N_A= {} \\n\".format(N_A)+ \" Min= {} \\n\".format(Min) + \" Max= {} \\n\".format(Max) + \\\n",
    "    \" NPoints= {} \\n\".format(NPoints) + \" &EndGeneral \\n\"\n",
    "    \n",
    "    potential_section = ''\n",
    "    for i in range(len(AtomPairs)):\n",
    "        single_pot = potentials[k]\n",
    "        header_section = \" &Potential \\n\" + \" Name= {}-{} \\n\".format(AtomPairs[i][0], AtomPairs[i][1]) + \\\n",
    "        \" Type = NB \\n\" \\\n",
    "        + \" Min= {} \\n\".format(Min) + \" Max= {} \\n\".format(Max) + \" NPoints= {} \\n\".format(NPoints) + \\\n",
    "        \" AtomTypes= {},{} \\n\".format(AtomPairs[i][0], AtomPairs[i][1])\n",
    "        \n",
    "        table_section = ' &Table \\n'\n",
    "        for j in range(len(rvalues)):\n",
    "            rval = format(rvalues[j], '.7f')\n",
    "            potval = format(single_pot[j], '.7f')\n",
    "            table_section = table_section + \"     \" + str(rval) + \"  \" + str(potval) + \" \\n\"\n",
    "            \n",
    "        table_section = table_section + \" &EndTable \\n\" + \" &EndPotential \\n\"\n",
    "        \n",
    "        potential_section = potential_section + header_section + table_section\n",
    "        \n",
    "    potfile = open('{}.pot'.format(filename), 'w')\n",
    "    potfile.write(general_section)\n",
    "    potfile.write(potential_section)\n",
    "    potfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wound-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    write_potential('Magic_Potentials/potential{}'.format(i), i)\n",
    "    pot = mt.ReadPot('MagiC_Potentials/potential{}.pot'.format(i), quiet = True)\n",
    "    mt.PotsExport(pot, MDEngine = 'LAMMPS', Rmaxtable = 15.5, PHImaxtable = 180.0, \\\n",
    "                    npoints = 2500, Umax = 6000.0, \\\n",
    "                    interpol = True, method = 'gauss', sigma = 0.5, noplot=True, hardcopy = False, \\\n",
    "                    filename = 'LAMMPS_Potentials/potential{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "grateful-falls",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: processors 1 1 1\n",
      "\n",
      "1: units real\n",
      "\n",
      "2: atom_style atomic\n",
      "\n",
      "3: \n",
      "\n",
      "4: region box block 0 40 0 40 0 40\n",
      "\n",
      "5: create_box 1 box\n",
      "\n",
      "6: create_atoms 1 random 20 2352 box\n",
      "\n",
      "7: mass 1 23\n",
      "\n",
      "8: \n",
      "\n",
      "9: pair_style table linear 1000\n",
      "\n",
      "10: pair_coeff 1 1 target_potential.Na_Na.table Na_Na\n",
      "\n",
      "11: \n",
      "\n",
      "12: velocity all create 0.5 157\n",
      "\n",
      "13: \n",
      "\n",
      "14: neighbor 0.5 bin\n",
      "\n",
      "15: neigh_modify every 20 delay 0 check no \n",
      "\n",
      "16: \n",
      "\n",
      "17: fix 1 all nvt temp 300 300 100\n",
      "\n",
      "18: dump id all xtc 100 NaNa.xtc\n",
      "\n",
      "19: \n",
      "\n",
      "20: timestep 1\n",
      "\n",
      "21: \n",
      "\n",
      "22: run 1000000\n"
     ]
    }
   ],
   "source": [
    "inp = open(\"Setup/NaNa.in\", \"r\")\n",
    "inp_lines = inp.readlines()\n",
    "inp.close\n",
    "for i in range(len(inp_lines)):\n",
    "    print('{}: '.format(i) + inp_lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "weird-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_input_xtc(i):\n",
    "    inp = open(\"Setup/NaNa.in\", \"r\")\n",
    "    inp_lines = inp.readlines()\n",
    "    inp.close\n",
    "    \n",
    "    inp_lines[10] = 'pair_coeff 1 1 LAMMPS_Potentials/potential{}.Na_Na.table Na_Na'.format(i)\n",
    "    \n",
    "    inp_lines[18] = 'dump id all xtc 100 LAMMPS_Trajectories/traj{}.xtc'.format(i)\n",
    "    \n",
    "    inp_lines[22] = 'run 300000'\n",
    "    \n",
    "    out = open('LAMMPS_Inputs/input{}.in'.format(i), 'w')\n",
    "    for obj in inp_lines:\n",
    "        out.write(obj)\n",
    "    out.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "framed-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    write_input_xtc(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "modified-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_input_lammpstrj(i):\n",
    "    inp = open(\"Setup/NaNa.in\", \"r\")\n",
    "    inp_lines = inp.readlines()\n",
    "    inp.close\n",
    "    \n",
    "    inp_lines[10] = 'pair_coeff 1 1 LAMMPS_Potentials/potential{}.Na_Na.table Na_Na'.format(i)\n",
    "    \n",
    "    inp_lines[18] = 'dump id all atom 100 LAMMPS_Trajectories_Test/traj{}.lammpstrj'.format(i)\n",
    "    \n",
    "    inp_lines[22] = 'run 300000'\n",
    "    \n",
    "    out = open('LAMMPS_Inputs_Test/input{}.in'.format(i), 'w')\n",
    "    for obj in inp_lines:\n",
    "        out.write(obj)\n",
    "    out.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "flush-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    write_input_lammpstrj(i)\n",
    "    os.system('lmp -in LAMMPS_Inputs_Test/input{}.in'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bored-actor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: &Parameters\n",
      "\n",
      "1:  TrajFile = NaNa.xtc\n",
      "\n",
      "2:  NMType = 1\n",
      "\n",
      "3:  NameMType = Na+.CG,\n",
      "\n",
      "4:  NMolMType = 20,\n",
      "\n",
      "5:  NFBEG = 1,\n",
      "\n",
      "6:  NFEND = 1,\n",
      "\n",
      "7:  ISTEP = 1, \n",
      "\n",
      "8:  OutputFile = NaNa.rdf\n",
      "\n",
      "9:  RMaxNB = 15.\n",
      "\n",
      "10:  RMaxB =10.0\n",
      "\n",
      "11:  ResolNB =0.05\n",
      "\n",
      "12:  ResolB=0.02\n",
      "\n",
      "13:  ResolA=1.0\n",
      "\n",
      "14: &ENDParameters\n",
      "\n",
      "15: \n",
      "\n",
      "16: &CGTypes\n",
      "\n",
      "17: Na: Na\n",
      "\n",
      "18: &EndCGTypes\n",
      "\n",
      "19: \n",
      "\n",
      "20: &RDFsNB\n",
      "\n",
      "21: Add: all\n",
      "\n",
      "22: &EndRDFsNB\n",
      "\n",
      "23: \n",
      "\n",
      "24: &RDFsB\n",
      "\n",
      "25: &EndRDFsB\n",
      "\n",
      "26: \n",
      "\n",
      "27: &RDFsA\n",
      "\n",
      "28: &EndRDFsA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp = open(\"NaNa.inp\", \"r\")\n",
    "inp_lines = inp.readlines()\n",
    "inp.close\n",
    "for i in range(len(inp_lines)):\n",
    "    print('{}: '.format(i) + inp_lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "southwest-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_rdf_inp(i):\n",
    "    inp = open(\"NaNa.inp\", \"r\")\n",
    "    inp_lines = inp.readlines()\n",
    "    inp.close\n",
    "\n",
    "    inp_lines[1] = ' TrajFile = LAMMPS_Trajectories/traj{}.xtc \\n'.format(i)\n",
    "    \n",
    "    inp_lines[8] = ' OutputFile = RDFs/rdf{}.rdf \\n'.format(i)\n",
    "    \n",
    "    out = open('RDF_Inputs/rdfinput{}.inp'.format(i), 'w')\n",
    "    for obj in inp_lines:\n",
    "        out.write(obj)\n",
    "    out.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "canadian-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    write_rdf_inp(i)\n",
    "    os.system('rdf.py -i RDF_Inputs/rdfinput{}.inp'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-march",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pressing-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdflist = []\n",
    "potlist = []\n",
    "for i in range(10000):\n",
    "    rdfval = mt.ReadRDF('RDFs/rdf{}.rdf'.format(i), quiet = True)\n",
    "    rdflist.append(rdfval)\n",
    "    potval = mt.ReadPot('Magic_Potentials/potential{}.pot'.format(i), quiet = True)\n",
    "    potlist.append(potval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "national-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "endless-facing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(rdflist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "outstanding-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdfs = []\n",
    "for i in range(len(rdflist)):\n",
    "    N = len(rdflist[i].DFs)\n",
    "    \n",
    "    for j in range(N):\n",
    "        holder = np.empty((300))\n",
    "        startindex = np.where(x == round(rdflist[i].DFs[j].x[0], 3))[0][0]\n",
    "        for k in range(300):\n",
    "            if k < startindex:\n",
    "                holder[k] = 0\n",
    "            elif k >= startindex and k-startindex < len(rdflist[i].DFs[j].x):\n",
    "                holder[k] = rdflist[i].DFs[j].y[k-startindex]\n",
    "            else:\n",
    "                holder[k] = 0\n",
    "        \n",
    "        new = np.zeros(len(input_rdf.DFs[0].x))\n",
    "        for l in range(1, len(new) + 1):\n",
    "            new[-l] = holder[-l]\n",
    "        \n",
    "        new = tf.constant(new)\n",
    "        rdfs.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "helpful-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "pots = []\n",
    "for i in range(len(potlist)):\n",
    "    N = len(rdflist[i].DFs)\n",
    "    \n",
    "    for j in range(N):\n",
    "        holder = np.empty((300))\n",
    "        \n",
    "        startindex = np.where(x == round(potlist[i].DFs[j].x[0], 3))[0][0]\n",
    "        for k in range(300):\n",
    "            if k < startindex:\n",
    "                holder[k] = 300 \n",
    "            elif k >= startindex and k-startindex < len(potlist[i].DFs[j].x):\n",
    "                holder[k] = potlist[i].DFs[j].y[k-startindex] + 30\n",
    "            else:\n",
    "                print(\"error\", i, j)\n",
    "        \n",
    "        new = np.zeros(len(potlist[0].DFs[0].x))\n",
    "        for l in range(1, len(new) + 1):\n",
    "            new[-l] = holder[-l]\n",
    "        \n",
    "        new = tf.constant(new)\n",
    "        pots.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "removable-ocean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(pots))\n",
    "print(len(rdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cellular-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.stack(rdfs)\n",
    "outputs = tf.stack(pots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "satisfied-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "interested-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 257)               66306     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 110)               28380     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 110)               12210     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 257)               28527     \n",
      "=================================================================\n",
      "Total params: 135,423\n",
      "Trainable params: 135,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(rdfs[0].shape[0], activation = 'selu', input_shape = rdfs[0].shape))\n",
    "model.add(Dense(110, activation = 'selu'))\n",
    "model.add(Dense(110, activation = 'selu'))\n",
    "\n",
    "model.add(Dense(pots[0].shape[0]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "temporal-setting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 69.2831 - val_loss: 3.4999\n",
      "Epoch 2/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 2.8961 - val_loss: 2.3731\n",
      "Epoch 3/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 2.3158 - val_loss: 2.1360\n",
      "Epoch 4/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 2.1410 - val_loss: 2.2227\n",
      "Epoch 5/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 2.0337 - val_loss: 1.9836\n",
      "Epoch 6/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 1.9602 - val_loss: 1.8122\n",
      "Epoch 7/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 1.8870 - val_loss: 1.7329\n",
      "Epoch 8/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 1.8905 - val_loss: 1.6821\n",
      "Epoch 9/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 1.7759 - val_loss: 1.7059\n",
      "Epoch 10/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 1.7113 - val_loss: 1.6726\n",
      "Epoch 11/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.6751 - val_loss: 1.5391\n",
      "Epoch 12/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 1.6166 - val_loss: 1.5189\n",
      "Epoch 13/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 1.5929 - val_loss: 1.5190\n",
      "Epoch 14/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 1.5590 - val_loss: 1.5093\n",
      "Epoch 15/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 1.5494 - val_loss: 1.4337\n",
      "Epoch 16/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 1.5483 - val_loss: 1.3675\n",
      "Epoch 17/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 1.4818 - val_loss: 1.3299\n",
      "Epoch 18/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.4355 - val_loss: 1.3655\n",
      "Epoch 19/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.4707 - val_loss: 1.2742\n",
      "Epoch 20/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 1.4035 - val_loss: 1.2632\n",
      "Epoch 21/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 1.3544 - val_loss: 1.2624\n",
      "Epoch 22/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.3475 - val_loss: 1.3671\n",
      "Epoch 23/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 1.3218 - val_loss: 1.2644\n",
      "Epoch 24/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 1.3503 - val_loss: 1.2744\n",
      "Epoch 25/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.3170 - val_loss: 1.4056\n",
      "Epoch 26/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 1.3287 - val_loss: 1.1397\n",
      "Epoch 27/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.3144 - val_loss: 1.2359\n",
      "Epoch 28/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 1.2709 - val_loss: 1.2065\n",
      "Epoch 29/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 1.2335 - val_loss: 1.1986\n",
      "Epoch 30/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 1.2196 - val_loss: 1.0941\n",
      "Epoch 31/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.2495 - val_loss: 1.1260\n",
      "Epoch 32/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 1.1790 - val_loss: 1.0718\n",
      "Epoch 33/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 1.1912 - val_loss: 1.1245\n",
      "Epoch 34/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 1.1987 - val_loss: 1.1119\n",
      "Epoch 35/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 1.1452 - val_loss: 1.0537\n",
      "Epoch 36/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 1.1025 - val_loss: 1.0183\n",
      "Epoch 37/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 1.1886 - val_loss: 1.0719\n",
      "Epoch 38/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 1.1306 - val_loss: 1.1286\n",
      "Epoch 39/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 1.1434 - val_loss: 0.9904\n",
      "Epoch 40/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 1.0747 - val_loss: 0.9826\n",
      "Epoch 41/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 1.1229 - val_loss: 1.0291\n",
      "Epoch 42/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 1.1103 - val_loss: 1.1587\n",
      "Epoch 43/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 1.0789 - val_loss: 0.9651\n",
      "Epoch 44/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 1.0237 - val_loss: 0.9550\n",
      "Epoch 45/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 1.0947 - val_loss: 0.9819\n",
      "Epoch 46/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 1.0086 - val_loss: 0.9649\n",
      "Epoch 47/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 1.0262 - val_loss: 0.9634\n",
      "Epoch 48/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.9892 - val_loss: 0.9633\n",
      "Epoch 49/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 1.0554 - val_loss: 0.9374\n",
      "Epoch 50/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 1.0089 - val_loss: 0.9099\n",
      "Epoch 51/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.9436 - val_loss: 0.8562\n",
      "Epoch 52/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.9742 - val_loss: 0.9341\n",
      "Epoch 53/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.9394 - val_loss: 0.8431\n",
      "Epoch 54/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.9180 - val_loss: 0.8556\n",
      "Epoch 55/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.9039 - val_loss: 0.8390\n",
      "Epoch 56/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.8996 - val_loss: 0.8686\n",
      "Epoch 57/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.9017 - val_loss: 0.7709\n",
      "Epoch 58/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.8778 - val_loss: 0.7866\n",
      "Epoch 59/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8794 - val_loss: 0.8920\n",
      "Epoch 60/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.8675 - val_loss: 0.8974\n",
      "Epoch 61/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.8493 - val_loss: 0.7820\n",
      "Epoch 62/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.8779 - val_loss: 0.8606\n",
      "Epoch 63/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.8870 - val_loss: 0.8332\n",
      "Epoch 64/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.8403 - val_loss: 0.7413\n",
      "Epoch 65/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.8016 - val_loss: 0.7466\n",
      "Epoch 66/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.8013 - val_loss: 0.9464\n",
      "Epoch 67/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.8698 - val_loss: 0.7929\n",
      "Epoch 68/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.7723 - val_loss: 0.7311\n",
      "Epoch 69/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.8153 - val_loss: 0.6994\n",
      "Epoch 70/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.7755 - val_loss: 0.7307\n",
      "Epoch 71/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.7549 - val_loss: 0.8251\n",
      "Epoch 72/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.7673 - val_loss: 0.7920\n",
      "Epoch 73/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.7740 - val_loss: 0.7050\n",
      "Epoch 74/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.6942 - val_loss: 0.6491\n",
      "Epoch 75/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.7553 - val_loss: 0.8171\n",
      "Epoch 76/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.7225 - val_loss: 0.6294\n",
      "Epoch 77/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.7053 - val_loss: 0.6657\n",
      "Epoch 78/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.7204 - val_loss: 0.6942\n",
      "Epoch 79/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.7096 - val_loss: 0.7741\n",
      "Epoch 80/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.7421 - val_loss: 0.6620\n",
      "Epoch 81/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6741 - val_loss: 0.7020\n",
      "Epoch 82/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.6845 - val_loss: 0.6697\n",
      "Epoch 83/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.6543 - val_loss: 0.6468\n",
      "Epoch 84/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.6869 - val_loss: 0.6303\n",
      "Epoch 85/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6788 - val_loss: 0.6225\n",
      "Epoch 86/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.7126 - val_loss: 0.6296\n",
      "Epoch 87/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.6743 - val_loss: 0.6408\n",
      "Epoch 88/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.7056 - val_loss: 0.6264\n",
      "Epoch 89/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.6720 - val_loss: 0.7513\n",
      "Epoch 90/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6498 - val_loss: 0.6108\n",
      "Epoch 91/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6380 - val_loss: 0.6603\n",
      "Epoch 92/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.6069 - val_loss: 0.5763\n",
      "Epoch 93/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6392 - val_loss: 0.6353\n",
      "Epoch 94/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6021 - val_loss: 0.6097\n",
      "Epoch 95/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.6375 - val_loss: 0.5922\n",
      "Epoch 96/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.5989 - val_loss: 0.5801\n",
      "Epoch 97/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6113 - val_loss: 0.6041\n",
      "Epoch 98/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.6204 - val_loss: 0.6918\n",
      "Epoch 99/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6592 - val_loss: 0.5736\n",
      "Epoch 100/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.6056 - val_loss: 0.6433\n",
      "Epoch 101/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5952 - val_loss: 0.5433\n",
      "Epoch 102/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5649 - val_loss: 0.5711\n",
      "Epoch 103/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.6034 - val_loss: 0.6384\n",
      "Epoch 104/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6840 - val_loss: 0.5412\n",
      "Epoch 105/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6087 - val_loss: 0.5847\n",
      "Epoch 106/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.6072 - val_loss: 0.5764\n",
      "Epoch 107/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5506 - val_loss: 0.5494\n",
      "Epoch 108/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6454 - val_loss: 0.5733\n",
      "Epoch 109/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5663 - val_loss: 0.5839\n",
      "Epoch 110/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6163 - val_loss: 0.5547\n",
      "Epoch 111/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5428 - val_loss: 0.5257\n",
      "Epoch 112/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5562 - val_loss: 0.6143\n",
      "Epoch 113/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5838 - val_loss: 0.5541\n",
      "Epoch 114/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5607 - val_loss: 0.6356\n",
      "Epoch 115/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.5749 - val_loss: 0.5875\n",
      "Epoch 116/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5899 - val_loss: 0.5295\n",
      "Epoch 117/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5623 - val_loss: 0.5200\n",
      "Epoch 118/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5501 - val_loss: 0.5336\n",
      "Epoch 119/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.5505 - val_loss: 0.7206\n",
      "Epoch 120/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6299 - val_loss: 0.6113\n",
      "Epoch 121/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5509 - val_loss: 0.5554\n",
      "Epoch 122/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5640 - val_loss: 0.5130\n",
      "Epoch 123/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5275 - val_loss: 0.6773\n",
      "Epoch 124/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5189 - val_loss: 0.5878\n",
      "Epoch 125/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5071 - val_loss: 0.5281\n",
      "Epoch 126/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5845 - val_loss: 0.5892\n",
      "Epoch 127/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.5993 - val_loss: 0.6026\n",
      "Epoch 128/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5075 - val_loss: 0.6184\n",
      "Epoch 129/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.4971 - val_loss: 0.5551\n",
      "Epoch 130/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5228 - val_loss: 0.5005\n",
      "Epoch 131/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5637 - val_loss: 0.8411\n",
      "Epoch 132/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5785 - val_loss: 0.5627\n",
      "Epoch 133/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5181 - val_loss: 0.5222\n",
      "Epoch 134/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5802 - val_loss: 0.5008\n",
      "Epoch 135/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4936 - val_loss: 0.5635\n",
      "Epoch 136/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5090 - val_loss: 0.5430\n",
      "Epoch 137/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5698 - val_loss: 0.5595\n",
      "Epoch 138/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.5058 - val_loss: 0.6460\n",
      "Epoch 139/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5226 - val_loss: 0.5104\n",
      "Epoch 140/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4994 - val_loss: 0.6611\n",
      "Epoch 141/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5506 - val_loss: 0.5288\n",
      "Epoch 142/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.5047 - val_loss: 0.6034\n",
      "Epoch 143/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5007 - val_loss: 0.5046\n",
      "Epoch 144/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5158 - val_loss: 0.4832\n",
      "Epoch 145/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.4772 - val_loss: 0.4961\n",
      "Epoch 146/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5124 - val_loss: 0.5469\n",
      "Epoch 147/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4986 - val_loss: 0.4910\n",
      "Epoch 148/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4802 - val_loss: 0.4964\n",
      "Epoch 149/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4931 - val_loss: 0.4971\n",
      "Epoch 150/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4883 - val_loss: 0.5132\n",
      "Epoch 151/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5102 - val_loss: 0.4733\n",
      "Epoch 152/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4647 - val_loss: 0.5050\n",
      "Epoch 153/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4811 - val_loss: 0.6035\n",
      "Epoch 154/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.4644 - val_loss: 0.5143\n",
      "Epoch 155/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4848 - val_loss: 0.4874\n",
      "Epoch 156/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.5118 - val_loss: 0.5496\n",
      "Epoch 157/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4705 - val_loss: 0.5046\n",
      "Epoch 158/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5122 - val_loss: 0.7803\n",
      "Epoch 159/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4952 - val_loss: 0.4994\n",
      "Epoch 160/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4801 - val_loss: 0.4856\n",
      "Epoch 161/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4911 - val_loss: 0.4516\n",
      "Epoch 162/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4715 - val_loss: 0.5241\n",
      "Epoch 163/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4968 - val_loss: 0.4837\n",
      "Epoch 164/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4730 - val_loss: 0.4878\n",
      "Epoch 165/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4570 - val_loss: 0.4629\n",
      "Epoch 166/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4511 - val_loss: 0.4845\n",
      "Epoch 167/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4797 - val_loss: 0.5566\n",
      "Epoch 168/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4596 - val_loss: 0.4890\n",
      "Epoch 169/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4367 - val_loss: 0.5189\n",
      "Epoch 170/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4940 - val_loss: 0.5657\n",
      "Epoch 171/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4799 - val_loss: 0.4679\n",
      "Epoch 172/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4624 - val_loss: 0.4814\n",
      "Epoch 173/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4399 - val_loss: 0.5356\n",
      "Epoch 174/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4548 - val_loss: 0.5133\n",
      "Epoch 175/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4707 - val_loss: 0.5002\n",
      "Epoch 176/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4641 - val_loss: 0.4974\n",
      "Epoch 177/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4539 - val_loss: 0.5036\n",
      "Epoch 178/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.4418 - val_loss: 0.6289\n",
      "Epoch 179/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4516 - val_loss: 0.5273\n",
      "Epoch 180/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4307 - val_loss: 0.5083\n",
      "Epoch 181/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.4647 - val_loss: 0.4770\n",
      "Epoch 182/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4247 - val_loss: 0.4416\n",
      "Epoch 183/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4682 - val_loss: 0.4913\n",
      "Epoch 184/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4313 - val_loss: 0.5024\n",
      "Epoch 185/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4272 - val_loss: 0.5267\n",
      "Epoch 186/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4266 - val_loss: 0.4709\n",
      "Epoch 187/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4754 - val_loss: 0.4770\n",
      "Epoch 188/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4641 - val_loss: 0.4522\n",
      "Epoch 189/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4415 - val_loss: 0.4753\n",
      "Epoch 190/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4464 - val_loss: 0.4817\n",
      "Epoch 191/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4071 - val_loss: 0.6740\n",
      "Epoch 192/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4872 - val_loss: 0.5762\n",
      "Epoch 193/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.4714 - val_loss: 0.5722\n",
      "Epoch 194/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4204 - val_loss: 0.4425\n",
      "Epoch 195/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4204 - val_loss: 0.4336\n",
      "Epoch 196/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4626 - val_loss: 0.4869\n",
      "Epoch 197/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4072 - val_loss: 0.5000\n",
      "Epoch 198/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5175 - val_loss: 0.4509\n",
      "Epoch 199/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.4305 - val_loss: 0.4341\n",
      "Epoch 200/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4159 - val_loss: 0.4809\n",
      "Epoch 201/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4509 - val_loss: 0.5159\n",
      "Epoch 202/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4297 - val_loss: 0.5252\n",
      "Epoch 203/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4431 - val_loss: 0.4496\n",
      "Epoch 204/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.4274 - val_loss: 0.4676\n",
      "Epoch 205/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.4277 - val_loss: 0.4583\n",
      "Epoch 206/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4175 - val_loss: 0.4375\n",
      "Epoch 207/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4357 - val_loss: 0.4777\n",
      "Epoch 208/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4426 - val_loss: 0.5890\n",
      "Epoch 209/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.4057 - val_loss: 0.4593\n",
      "Epoch 210/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4542 - val_loss: 0.4492\n",
      "Epoch 211/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3896 - val_loss: 0.4269\n",
      "Epoch 212/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4466 - val_loss: 0.4878\n",
      "Epoch 213/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.4283 - val_loss: 0.4685\n",
      "Epoch 214/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4034 - val_loss: 0.4476\n",
      "Epoch 215/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.4287 - val_loss: 0.4662\n",
      "Epoch 216/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4632 - val_loss: 0.5341\n",
      "Epoch 217/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4092 - val_loss: 0.4372\n",
      "Epoch 218/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3973 - val_loss: 0.4657\n",
      "Epoch 219/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4010 - val_loss: 0.4750\n",
      "Epoch 220/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.4552 - val_loss: 0.4915\n",
      "Epoch 221/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.4047 - val_loss: 0.4347\n",
      "Epoch 222/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4458 - val_loss: 0.5141\n",
      "Epoch 223/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4466 - val_loss: 0.4305\n",
      "Epoch 224/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3939 - val_loss: 0.4793\n",
      "Epoch 225/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4086 - val_loss: 0.4453\n",
      "Epoch 226/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3973 - val_loss: 0.4800\n",
      "Epoch 227/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.4027 - val_loss: 0.4831\n",
      "Epoch 228/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4104 - val_loss: 0.5131\n",
      "Epoch 229/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4583 - val_loss: 0.5431\n",
      "Epoch 230/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4232 - val_loss: 0.4716\n",
      "Epoch 231/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3824 - val_loss: 0.5305\n",
      "Epoch 232/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4140 - val_loss: 0.5394\n",
      "Epoch 233/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4064 - val_loss: 0.4297\n",
      "Epoch 234/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.4074 - val_loss: 0.5458\n",
      "Epoch 235/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3701 - val_loss: 0.4327\n",
      "Epoch 236/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.4140 - val_loss: 0.4455\n",
      "Epoch 237/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3978 - val_loss: 0.5424\n",
      "Epoch 238/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.4045 - val_loss: 0.4329\n",
      "Epoch 239/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4091 - val_loss: 0.4607\n",
      "Epoch 240/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.4311 - val_loss: 0.5252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3961 - val_loss: 0.4567\n",
      "Epoch 242/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3902 - val_loss: 0.5793\n",
      "Epoch 243/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.4185 - val_loss: 0.4687\n",
      "Epoch 244/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4128 - val_loss: 0.4853\n",
      "Epoch 245/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3683 - val_loss: 0.4279\n",
      "Epoch 246/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3920 - val_loss: 0.5236\n",
      "Epoch 247/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4117 - val_loss: 0.4403\n",
      "Epoch 248/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4425 - val_loss: 0.5511\n",
      "Epoch 249/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3779 - val_loss: 0.4630\n",
      "Epoch 250/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.4063 - val_loss: 0.4353\n",
      "Epoch 251/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3706 - val_loss: 0.6477\n",
      "Epoch 252/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3904 - val_loss: 0.4314\n",
      "Epoch 253/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3629 - val_loss: 0.4925\n",
      "Epoch 254/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4164 - val_loss: 0.4318\n",
      "Epoch 255/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3684 - val_loss: 0.4514\n",
      "Epoch 256/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3849 - val_loss: 0.4220\n",
      "Epoch 257/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3893 - val_loss: 0.4270\n",
      "Epoch 258/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3789 - val_loss: 0.4895\n",
      "Epoch 259/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3643 - val_loss: 0.4622\n",
      "Epoch 260/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.3967 - val_loss: 0.4577\n",
      "Epoch 261/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3818 - val_loss: 0.5981\n",
      "Epoch 262/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4147 - val_loss: 0.5256\n",
      "Epoch 263/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4194 - val_loss: 0.4480\n",
      "Epoch 264/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3726 - val_loss: 0.4960\n",
      "Epoch 265/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3669 - val_loss: 0.5335\n",
      "Epoch 266/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3936 - val_loss: 0.4935\n",
      "Epoch 267/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3842 - val_loss: 0.4739\n",
      "Epoch 268/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3565 - val_loss: 0.4516\n",
      "Epoch 269/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3562 - val_loss: 0.4427\n",
      "Epoch 270/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3622 - val_loss: 0.4542\n",
      "Epoch 271/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4229 - val_loss: 0.4321\n",
      "Epoch 272/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3868 - val_loss: 0.4697\n",
      "Epoch 273/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3630 - val_loss: 0.4227\n",
      "Epoch 274/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3768 - val_loss: 0.5131\n",
      "Epoch 275/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3803 - val_loss: 0.4953\n",
      "Epoch 276/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4027 - val_loss: 0.4598\n",
      "Epoch 277/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3736 - val_loss: 0.4333\n",
      "Epoch 278/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3932 - val_loss: 0.4582\n",
      "Epoch 279/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3759 - val_loss: 0.5169\n",
      "Epoch 280/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3801 - val_loss: 0.4683\n",
      "Epoch 281/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4164 - val_loss: 0.4500\n",
      "Epoch 282/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3705 - val_loss: 0.4151\n",
      "Epoch 283/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3574 - val_loss: 0.4641\n",
      "Epoch 284/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3630 - val_loss: 0.4276\n",
      "Epoch 285/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3619 - val_loss: 0.4607\n",
      "Epoch 286/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3594 - val_loss: 0.4350\n",
      "Epoch 287/2000\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.3809 - val_loss: 0.4258\n",
      "Epoch 288/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.3642 - val_loss: 0.4778\n",
      "Epoch 289/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3855 - val_loss: 0.4774\n",
      "Epoch 290/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.3472 - val_loss: 0.5140\n",
      "Epoch 291/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4483\n",
      "Epoch 292/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3882 - val_loss: 0.4592\n",
      "Epoch 293/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3648 - val_loss: 0.5162\n",
      "Epoch 294/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3458 - val_loss: 0.4882\n",
      "Epoch 295/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3351 - val_loss: 0.4556\n",
      "Epoch 296/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3666 - val_loss: 0.4921\n",
      "Epoch 297/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3569 - val_loss: 0.4134\n",
      "Epoch 298/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3586 - val_loss: 0.5375\n",
      "Epoch 299/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3701 - val_loss: 0.4653\n",
      "Epoch 300/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3574 - val_loss: 0.4776\n",
      "Epoch 301/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3396 - val_loss: 0.5623\n",
      "Epoch 302/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4183 - val_loss: 0.5061\n",
      "Epoch 303/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3666 - val_loss: 0.4177\n",
      "Epoch 304/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3503 - val_loss: 0.4496\n",
      "Epoch 305/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3170 - val_loss: 0.4778\n",
      "Epoch 306/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3479 - val_loss: 0.4509\n",
      "Epoch 307/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3572 - val_loss: 0.4420\n",
      "Epoch 308/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4134 - val_loss: 0.5336\n",
      "Epoch 309/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3838 - val_loss: 0.4922\n",
      "Epoch 310/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3514 - val_loss: 0.5086\n",
      "Epoch 311/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3777 - val_loss: 0.5163\n",
      "Epoch 312/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3385 - val_loss: 0.4158\n",
      "Epoch 313/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3961 - val_loss: 0.5014\n",
      "Epoch 314/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3714 - val_loss: 0.4540\n",
      "Epoch 315/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3431 - val_loss: 0.4998\n",
      "Epoch 316/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.3674 - val_loss: 0.4756\n",
      "Epoch 317/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3405 - val_loss: 0.4454\n",
      "Epoch 318/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3576 - val_loss: 0.4511\n",
      "Epoch 319/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3473 - val_loss: 0.4696\n",
      "Epoch 320/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3665 - val_loss: 0.4353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3324 - val_loss: 0.4211\n",
      "Epoch 322/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3318 - val_loss: 0.4427\n",
      "Epoch 323/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3900 - val_loss: 0.4443\n",
      "Epoch 324/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3486 - val_loss: 0.4443\n",
      "Epoch 325/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3466 - val_loss: 0.4662\n",
      "Epoch 326/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3984 - val_loss: 0.4221\n",
      "Epoch 327/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3524 - val_loss: 0.4200\n",
      "Epoch 328/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3916 - val_loss: 0.5091\n",
      "Epoch 329/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3959 - val_loss: 0.4480\n",
      "Epoch 330/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3587 - val_loss: 0.4557\n",
      "Epoch 331/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4011 - val_loss: 0.4316\n",
      "Epoch 332/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3216 - val_loss: 0.4686\n",
      "Epoch 333/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3504 - val_loss: 0.4287\n",
      "Epoch 334/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.3568 - val_loss: 0.4687\n",
      "Epoch 335/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3749 - val_loss: 0.5246\n",
      "Epoch 336/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3721 - val_loss: 0.4367\n",
      "Epoch 337/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3279 - val_loss: 0.4208\n",
      "Epoch 338/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3120 - val_loss: 0.4562\n",
      "Epoch 339/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3661 - val_loss: 0.4865\n",
      "Epoch 340/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3448 - val_loss: 0.4656\n",
      "Epoch 341/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4358\n",
      "Epoch 342/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3418 - val_loss: 0.4226\n",
      "Epoch 343/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3303 - val_loss: 0.4260\n",
      "Epoch 344/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.3650 - val_loss: 0.5828\n",
      "Epoch 345/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3480 - val_loss: 0.5496\n",
      "Epoch 346/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3742 - val_loss: 0.4391\n",
      "Epoch 347/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3397 - val_loss: 0.4472\n",
      "Epoch 348/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3168 - val_loss: 0.4462\n",
      "Epoch 349/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3276 - val_loss: 0.4949\n",
      "Epoch 350/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3660 - val_loss: 0.5224\n",
      "Epoch 351/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3620 - val_loss: 0.5095\n",
      "Epoch 352/2000\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.3327 - val_loss: 0.4076\n",
      "Epoch 353/2000\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.3152 - val_loss: 0.4211\n",
      "Epoch 354/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3389 - val_loss: 0.4831\n",
      "Epoch 355/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3090 - val_loss: 0.4557\n",
      "Epoch 356/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3274 - val_loss: 0.4122\n",
      "Epoch 357/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3406 - val_loss: 0.4622\n",
      "Epoch 358/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3453 - val_loss: 0.5144\n",
      "Epoch 359/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3194 - val_loss: 0.4633\n",
      "Epoch 360/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3540 - val_loss: 0.5011\n",
      "Epoch 361/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3174 - val_loss: 0.4805\n",
      "Epoch 362/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4167 - val_loss: 0.4407\n",
      "Epoch 363/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3253 - val_loss: 0.4223\n",
      "Epoch 364/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3154 - val_loss: 0.4410\n",
      "Epoch 365/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3759 - val_loss: 0.4447\n",
      "Epoch 366/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3100 - val_loss: 0.4279\n",
      "Epoch 367/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3100 - val_loss: 0.4245\n",
      "Epoch 368/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3149 - val_loss: 0.4035\n",
      "Epoch 369/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3510 - val_loss: 0.4467\n",
      "Epoch 370/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3178 - val_loss: 0.4602\n",
      "Epoch 371/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3509 - val_loss: 0.4793\n",
      "Epoch 372/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3131 - val_loss: 0.4315\n",
      "Epoch 373/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3006 - val_loss: 0.4377\n",
      "Epoch 374/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3512 - val_loss: 0.4744\n",
      "Epoch 375/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3161 - val_loss: 0.4341\n",
      "Epoch 376/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3026 - val_loss: 0.5757\n",
      "Epoch 377/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3592 - val_loss: 0.4161\n",
      "Epoch 378/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3284 - val_loss: 0.4317\n",
      "Epoch 379/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3756 - val_loss: 0.4801\n",
      "Epoch 380/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3268 - val_loss: 0.4290\n",
      "Epoch 381/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3396 - val_loss: 0.4265\n",
      "Epoch 382/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3153 - val_loss: 0.4550\n",
      "Epoch 383/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3165 - val_loss: 0.4267\n",
      "Epoch 384/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3305 - val_loss: 0.4363\n",
      "Epoch 385/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3152 - val_loss: 0.4560\n",
      "Epoch 386/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3229 - val_loss: 0.4668\n",
      "Epoch 387/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3211 - val_loss: 0.4056\n",
      "Epoch 388/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3082 - val_loss: 0.4200\n",
      "Epoch 389/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3127 - val_loss: 0.4151\n",
      "Epoch 390/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2918 - val_loss: 0.4839\n",
      "Epoch 391/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.4103 - val_loss: 0.4442\n",
      "Epoch 392/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3886 - val_loss: 0.4433\n",
      "Epoch 393/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3917 - val_loss: 0.4623\n",
      "Epoch 394/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3419 - val_loss: 0.4157\n",
      "Epoch 395/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3055 - val_loss: 0.4205\n",
      "Epoch 396/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3004 - val_loss: 0.4083\n",
      "Epoch 397/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3100 - val_loss: 0.4468\n",
      "Epoch 398/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3385 - val_loss: 0.4682\n",
      "Epoch 399/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2934 - val_loss: 0.4391\n",
      "Epoch 400/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3064 - val_loss: 0.4407\n",
      "Epoch 401/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3294 - val_loss: 0.5313\n",
      "Epoch 402/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3341 - val_loss: 0.4350\n",
      "Epoch 403/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3083 - val_loss: 0.4301\n",
      "Epoch 404/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3276 - val_loss: 0.4207\n",
      "Epoch 405/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3008 - val_loss: 0.4413\n",
      "Epoch 406/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3472 - val_loss: 0.4467\n",
      "Epoch 407/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3154 - val_loss: 0.4718\n",
      "Epoch 408/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3036 - val_loss: 0.5039\n",
      "Epoch 409/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3255 - val_loss: 0.4218\n",
      "Epoch 410/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2912 - val_loss: 0.3937\n",
      "Epoch 411/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3299 - val_loss: 0.4556\n",
      "Epoch 412/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3430 - val_loss: 0.4985\n",
      "Epoch 413/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3221 - val_loss: 0.4459\n",
      "Epoch 414/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3107 - val_loss: 0.4150\n",
      "Epoch 415/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3292 - val_loss: 0.4196\n",
      "Epoch 416/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3132 - val_loss: 0.4252\n",
      "Epoch 417/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3084 - val_loss: 0.4253\n",
      "Epoch 418/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3055 - val_loss: 0.4446\n",
      "Epoch 419/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2986 - val_loss: 0.4406\n",
      "Epoch 420/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3101 - val_loss: 0.4443\n",
      "Epoch 421/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3152 - val_loss: 0.4587\n",
      "Epoch 422/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3039 - val_loss: 0.4210\n",
      "Epoch 423/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3066 - val_loss: 0.5361\n",
      "Epoch 424/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3338 - val_loss: 0.4708\n",
      "Epoch 425/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3081 - val_loss: 0.4345\n",
      "Epoch 426/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3242 - val_loss: 0.3962\n",
      "Epoch 427/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2877 - val_loss: 0.4217\n",
      "Epoch 428/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2973 - val_loss: 0.4803\n",
      "Epoch 429/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3901 - val_loss: 0.4450\n",
      "Epoch 430/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3070 - val_loss: 0.5807\n",
      "Epoch 431/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3485 - val_loss: 0.4052\n",
      "Epoch 432/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3163 - val_loss: 0.5040\n",
      "Epoch 433/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2971 - val_loss: 0.5621\n",
      "Epoch 434/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3132 - val_loss: 0.4474\n",
      "Epoch 435/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3422 - val_loss: 0.4727\n",
      "Epoch 436/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3170 - val_loss: 0.4006\n",
      "Epoch 437/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3088 - val_loss: 0.4279\n",
      "Epoch 438/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3268 - val_loss: 0.4824\n",
      "Epoch 439/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3023 - val_loss: 0.4399\n",
      "Epoch 440/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3027 - val_loss: 0.4417\n",
      "Epoch 441/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3007 - val_loss: 0.3948\n",
      "Epoch 442/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2863 - val_loss: 0.4128\n",
      "Epoch 443/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2950 - val_loss: 0.4193\n",
      "Epoch 444/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2934 - val_loss: 0.4128\n",
      "Epoch 445/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3491 - val_loss: 0.5319\n",
      "Epoch 446/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3792 - val_loss: 0.4401\n",
      "Epoch 447/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3278 - val_loss: 0.4242\n",
      "Epoch 448/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3031 - val_loss: 0.4867\n",
      "Epoch 449/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3069 - val_loss: 0.4888\n",
      "Epoch 450/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2919 - val_loss: 0.4434\n",
      "Epoch 451/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3176 - val_loss: 0.5440\n",
      "Epoch 452/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3255 - val_loss: 0.4452\n",
      "Epoch 453/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2981 - val_loss: 0.4618\n",
      "Epoch 454/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2899 - val_loss: 0.3969\n",
      "Epoch 455/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.2913 - val_loss: 0.4657\n",
      "Epoch 456/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3224 - val_loss: 0.4717\n",
      "Epoch 457/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3246 - val_loss: 0.4585\n",
      "Epoch 458/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3244 - val_loss: 0.4401\n",
      "Epoch 459/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2910 - val_loss: 0.4354\n",
      "Epoch 460/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3064 - val_loss: 0.4229\n",
      "Epoch 461/2000\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.2823 - val_loss: 0.4054\n",
      "Epoch 462/2000\n",
      "80/80 [==============================] - 2s 25ms/step - loss: 0.2834 - val_loss: 0.4050\n",
      "Epoch 463/2000\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.3117 - val_loss: 0.4358\n",
      "Epoch 464/2000\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.2866 - val_loss: 0.4158\n",
      "Epoch 465/2000\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.2945 - val_loss: 0.4291\n",
      "Epoch 466/2000\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.3234 - val_loss: 0.4263\n",
      "Epoch 467/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2797 - val_loss: 0.3965\n",
      "Epoch 468/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2786 - val_loss: 0.4224\n",
      "Epoch 469/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2815 - val_loss: 0.4065\n",
      "Epoch 470/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2815 - val_loss: 0.4023\n",
      "Epoch 471/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2869 - val_loss: 0.4492\n",
      "Epoch 472/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3501 - val_loss: 0.4513\n",
      "Epoch 473/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2906 - val_loss: 0.4208\n",
      "Epoch 474/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3120 - val_loss: 0.4434\n",
      "Epoch 475/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3042 - val_loss: 0.4876\n",
      "Epoch 476/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3663 - val_loss: 0.4725\n",
      "Epoch 477/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3494 - val_loss: 0.4784\n",
      "Epoch 478/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2960 - val_loss: 0.4099\n",
      "Epoch 479/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2774 - val_loss: 0.4050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2740 - val_loss: 0.4761\n",
      "Epoch 481/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2831 - val_loss: 0.4391\n",
      "Epoch 482/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2713 - val_loss: 0.4163\n",
      "Epoch 483/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3037 - val_loss: 0.4597\n",
      "Epoch 484/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3218 - val_loss: 0.5341\n",
      "Epoch 485/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2977 - val_loss: 0.4358\n",
      "Epoch 486/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2748 - val_loss: 0.4347\n",
      "Epoch 487/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2925 - val_loss: 0.4742\n",
      "Epoch 488/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3179 - val_loss: 0.4482\n",
      "Epoch 489/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2798 - val_loss: 0.4252\n",
      "Epoch 490/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2903 - val_loss: 0.4513\n",
      "Epoch 491/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3016 - val_loss: 0.4262\n",
      "Epoch 492/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2702 - val_loss: 0.4060\n",
      "Epoch 493/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3134 - val_loss: 0.4462\n",
      "Epoch 494/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3295 - val_loss: 0.4317\n",
      "Epoch 495/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2811 - val_loss: 0.4357\n",
      "Epoch 496/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2976 - val_loss: 0.4821\n",
      "Epoch 497/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2919 - val_loss: 0.4192\n",
      "Epoch 498/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2910 - val_loss: 0.4782\n",
      "Epoch 499/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4093 - val_loss: 0.4377\n",
      "Epoch 500/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3627 - val_loss: 0.4660\n",
      "Epoch 501/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2905 - val_loss: 0.4407\n",
      "Epoch 502/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2998 - val_loss: 0.3996\n",
      "Epoch 503/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2835 - val_loss: 0.4493\n",
      "Epoch 504/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2922 - val_loss: 0.4583\n",
      "Epoch 505/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2794 - val_loss: 0.4432\n",
      "Epoch 506/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2762 - val_loss: 0.4492\n",
      "Epoch 507/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2983 - val_loss: 0.4774\n",
      "Epoch 508/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2764 - val_loss: 0.4047\n",
      "Epoch 509/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2672 - val_loss: 0.3914\n",
      "Epoch 510/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2733 - val_loss: 0.4694\n",
      "Epoch 511/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2667 - val_loss: 0.4206\n",
      "Epoch 512/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2748 - val_loss: 0.4122\n",
      "Epoch 513/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3509 - val_loss: 0.4983\n",
      "Epoch 514/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2869 - val_loss: 0.3983\n",
      "Epoch 515/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2797 - val_loss: 0.4468\n",
      "Epoch 516/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2842 - val_loss: 0.4143\n",
      "Epoch 517/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2675 - val_loss: 0.4649\n",
      "Epoch 518/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3128 - val_loss: 0.5248\n",
      "Epoch 519/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.3304 - val_loss: 0.4201\n",
      "Epoch 520/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2868 - val_loss: 0.4183\n",
      "Epoch 521/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2808 - val_loss: 0.4153\n",
      "Epoch 522/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3142 - val_loss: 0.4410\n",
      "Epoch 523/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2738 - val_loss: 0.4487\n",
      "Epoch 524/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2873 - val_loss: 0.4124\n",
      "Epoch 525/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2798 - val_loss: 0.4463\n",
      "Epoch 526/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2860 - val_loss: 0.4067\n",
      "Epoch 527/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2807 - val_loss: 0.4078\n",
      "Epoch 528/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2936 - val_loss: 0.4251\n",
      "Epoch 529/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2676 - val_loss: 0.4253\n",
      "Epoch 530/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2607 - val_loss: 0.4162\n",
      "Epoch 531/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3212 - val_loss: 0.4177\n",
      "Epoch 532/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3139 - val_loss: 0.4279\n",
      "Epoch 533/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2874 - val_loss: 0.4189\n",
      "Epoch 534/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2854 - val_loss: 0.4811\n",
      "Epoch 535/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3042 - val_loss: 0.4039\n",
      "Epoch 536/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2699 - val_loss: 0.3882\n",
      "Epoch 537/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2979 - val_loss: 0.4185\n",
      "Epoch 538/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2962 - val_loss: 0.4489\n",
      "Epoch 539/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2954 - val_loss: 0.4014\n",
      "Epoch 540/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2694 - val_loss: 0.4151\n",
      "Epoch 541/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2782 - val_loss: 0.5244\n",
      "Epoch 542/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3045 - val_loss: 0.4557\n",
      "Epoch 543/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2804 - val_loss: 0.4153\n",
      "Epoch 544/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2888 - val_loss: 0.4431\n",
      "Epoch 545/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2633 - val_loss: 0.4173\n",
      "Epoch 546/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2617 - val_loss: 0.4238\n",
      "Epoch 547/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2747 - val_loss: 0.4389\n",
      "Epoch 548/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2950 - val_loss: 0.4346\n",
      "Epoch 549/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2745 - val_loss: 0.4228\n",
      "Epoch 550/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2606 - val_loss: 0.4069\n",
      "Epoch 551/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2709 - val_loss: 0.4403\n",
      "Epoch 552/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2800 - val_loss: 0.4123\n",
      "Epoch 553/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2730 - val_loss: 0.4175\n",
      "Epoch 554/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2612 - val_loss: 0.4084\n",
      "Epoch 555/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2911 - val_loss: 0.4136\n",
      "Epoch 556/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2848 - val_loss: 0.4270\n",
      "Epoch 557/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2935 - val_loss: 0.5514\n",
      "Epoch 558/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2871 - val_loss: 0.4468\n",
      "Epoch 559/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2980 - val_loss: 0.4023\n",
      "Epoch 560/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2783 - val_loss: 0.4067\n",
      "Epoch 561/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2768 - val_loss: 0.4273\n",
      "Epoch 562/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2914 - val_loss: 0.5088\n",
      "Epoch 563/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3075 - val_loss: 0.4360\n",
      "Epoch 564/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2677 - val_loss: 0.4996\n",
      "Epoch 565/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2975 - val_loss: 0.4588\n",
      "Epoch 566/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2811 - val_loss: 0.4404\n",
      "Epoch 567/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2896 - val_loss: 0.5078\n",
      "Epoch 568/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3160 - val_loss: 0.4053\n",
      "Epoch 569/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2625 - val_loss: 0.4573\n",
      "Epoch 570/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3429 - val_loss: 0.4251\n",
      "Epoch 571/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2684 - val_loss: 0.4837\n",
      "Epoch 572/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2913 - val_loss: 0.4978\n",
      "Epoch 573/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2799 - val_loss: 0.4043\n",
      "Epoch 574/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2596 - val_loss: 0.4259\n",
      "Epoch 575/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2720 - val_loss: 0.4147\n",
      "Epoch 576/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2503 - val_loss: 0.4072\n",
      "Epoch 577/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3058 - val_loss: 0.4595\n",
      "Epoch 578/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2637 - val_loss: 0.4409\n",
      "Epoch 579/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2574 - val_loss: 0.3987\n",
      "Epoch 580/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2604 - val_loss: 0.4285\n",
      "Epoch 581/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2789 - val_loss: 0.6828\n",
      "Epoch 582/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.3180 - val_loss: 0.3908\n",
      "Epoch 583/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2557 - val_loss: 0.4458\n",
      "Epoch 584/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2621 - val_loss: 0.4067\n",
      "Epoch 585/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.2568 - val_loss: 0.4069\n",
      "Epoch 586/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2473 - val_loss: 0.4114\n",
      "Epoch 587/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2777 - val_loss: 0.4736\n",
      "Epoch 588/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2580 - val_loss: 0.4118\n",
      "Epoch 589/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2633 - val_loss: 0.4411\n",
      "Epoch 590/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2646 - val_loss: 0.4203\n",
      "Epoch 591/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2869 - val_loss: 0.4086\n",
      "Epoch 592/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2941 - val_loss: 0.4132\n",
      "Epoch 593/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2819 - val_loss: 0.4320\n",
      "Epoch 594/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3061 - val_loss: 0.4314\n",
      "Epoch 595/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2869 - val_loss: 0.4321\n",
      "Epoch 596/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2675 - val_loss: 0.4038\n",
      "Epoch 597/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2424 - val_loss: 0.4028\n",
      "Epoch 598/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2701 - val_loss: 0.4344\n",
      "Epoch 599/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2616 - val_loss: 0.4132\n",
      "Epoch 600/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2737 - val_loss: 0.4026\n",
      "Epoch 601/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2656 - val_loss: 0.4167\n",
      "Epoch 602/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2804 - val_loss: 0.4086\n",
      "Epoch 603/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3050 - val_loss: 0.4319\n",
      "Epoch 604/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2943 - val_loss: 0.4192\n",
      "Epoch 605/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2494 - val_loss: 0.4008\n",
      "Epoch 606/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2531 - val_loss: 0.4319\n",
      "Epoch 607/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2863 - val_loss: 0.4352\n",
      "Epoch 608/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2891 - val_loss: 0.3933\n",
      "Epoch 609/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2509 - val_loss: 0.4183\n",
      "Epoch 610/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2632 - val_loss: 0.4465\n",
      "Epoch 611/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2396 - val_loss: 0.4221\n",
      "Epoch 612/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2674 - val_loss: 0.4138\n",
      "Epoch 613/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2829 - val_loss: 0.4561\n",
      "Epoch 614/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2779 - val_loss: 0.4057\n",
      "Epoch 615/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2583 - val_loss: 0.4330\n",
      "Epoch 616/2000\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.2770 - val_loss: 0.4769\n",
      "Epoch 617/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3005 - val_loss: 0.4379\n",
      "Epoch 618/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2495 - val_loss: 0.4101\n",
      "Epoch 619/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2742 - val_loss: 0.4432\n",
      "Epoch 620/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2842 - val_loss: 0.4141\n",
      "Epoch 621/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2750 - val_loss: 0.4456\n",
      "Epoch 622/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2886 - val_loss: 0.4311\n",
      "Epoch 623/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2672 - val_loss: 0.4338\n",
      "Epoch 624/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2489 - val_loss: 0.3915\n",
      "Epoch 625/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2626 - val_loss: 0.3932\n",
      "Epoch 626/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2673 - val_loss: 0.4592\n",
      "Epoch 627/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2655 - val_loss: 0.4000\n",
      "Epoch 628/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2749 - val_loss: 0.4714\n",
      "Epoch 629/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2716 - val_loss: 0.4255\n",
      "Epoch 630/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2425 - val_loss: 0.3894\n",
      "Epoch 631/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2675 - val_loss: 0.4882\n",
      "Epoch 632/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.3144 - val_loss: 0.4159\n",
      "Epoch 633/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2541 - val_loss: 0.4007\n",
      "Epoch 634/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2728 - val_loss: 0.4063\n",
      "Epoch 635/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2414 - val_loss: 0.4825\n",
      "Epoch 636/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2801 - val_loss: 0.4085\n",
      "Epoch 637/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.2579 - val_loss: 0.4078\n",
      "Epoch 638/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3061 - val_loss: 0.4250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 639/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2462 - val_loss: 0.3895\n",
      "Epoch 640/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2641 - val_loss: 0.4256\n",
      "Epoch 641/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2720 - val_loss: 0.3921\n",
      "Epoch 642/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2439 - val_loss: 0.4155\n",
      "Epoch 643/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2766 - val_loss: 0.4050\n",
      "Epoch 644/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2552 - val_loss: 0.4014\n",
      "Epoch 645/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2526 - val_loss: 0.4006\n",
      "Epoch 646/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2931 - val_loss: 0.4782\n",
      "Epoch 647/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2832 - val_loss: 0.4521\n",
      "Epoch 648/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2919 - val_loss: 0.4028\n",
      "Epoch 649/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2518 - val_loss: 0.4537\n",
      "Epoch 650/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2819 - val_loss: 0.4700\n",
      "Epoch 651/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2522 - val_loss: 0.4074\n",
      "Epoch 652/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2537 - val_loss: 0.3931\n",
      "Epoch 653/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2589 - val_loss: 0.4106\n",
      "Epoch 654/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2634 - val_loss: 0.4033\n",
      "Epoch 655/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2355 - val_loss: 0.4105\n",
      "Epoch 656/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2794 - val_loss: 0.4367\n",
      "Epoch 657/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2833 - val_loss: 0.4334\n",
      "Epoch 658/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2708 - val_loss: 0.4275\n",
      "Epoch 659/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2609 - val_loss: 0.4259\n",
      "Epoch 660/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2814 - val_loss: 0.4539\n",
      "Epoch 661/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2776 - val_loss: 0.4510\n",
      "Epoch 662/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2905 - val_loss: 0.4098\n",
      "Epoch 663/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2646 - val_loss: 0.4159\n",
      "Epoch 664/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2760 - val_loss: 0.4014\n",
      "Epoch 665/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2469 - val_loss: 0.4070\n",
      "Epoch 666/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3059 - val_loss: 0.4348\n",
      "Epoch 667/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2532 - val_loss: 0.3929\n",
      "Epoch 668/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2622 - val_loss: 0.5284\n",
      "Epoch 669/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2847 - val_loss: 0.3919\n",
      "Epoch 670/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2765 - val_loss: 0.3942\n",
      "Epoch 671/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2384 - val_loss: 0.4080\n",
      "Epoch 672/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2657 - val_loss: 0.4189\n",
      "Epoch 673/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2543 - val_loss: 0.4501\n",
      "Epoch 674/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3010 - val_loss: 0.4169\n",
      "Epoch 675/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2612 - val_loss: 0.3919\n",
      "Epoch 676/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2301 - val_loss: 0.4120\n",
      "Epoch 677/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2654 - val_loss: 0.4189\n",
      "Epoch 678/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2419 - val_loss: 0.3847\n",
      "Epoch 679/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2469 - val_loss: 0.4187\n",
      "Epoch 680/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2750 - val_loss: 0.4479\n",
      "Epoch 681/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2474 - val_loss: 0.4213\n",
      "Epoch 682/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2466 - val_loss: 0.4441\n",
      "Epoch 683/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2541 - val_loss: 0.4910\n",
      "Epoch 684/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2581 - val_loss: 0.4580\n",
      "Epoch 685/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2460 - val_loss: 0.4136\n",
      "Epoch 686/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2397 - val_loss: 0.4511\n",
      "Epoch 687/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2606 - val_loss: 0.4103\n",
      "Epoch 688/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2881 - val_loss: 0.4115\n",
      "Epoch 689/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2280 - val_loss: 0.4748\n",
      "Epoch 690/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2508 - val_loss: 0.4116\n",
      "Epoch 691/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2481 - val_loss: 0.3961\n",
      "Epoch 692/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2283 - val_loss: 0.3962\n",
      "Epoch 693/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2397 - val_loss: 0.3830\n",
      "Epoch 694/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2347 - val_loss: 0.3997\n",
      "Epoch 695/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2692 - val_loss: 0.5221\n",
      "Epoch 696/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.3013 - val_loss: 0.4106\n",
      "Epoch 697/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2860 - val_loss: 0.4621\n",
      "Epoch 698/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2741 - val_loss: 0.4045\n",
      "Epoch 699/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2371 - val_loss: 0.4505\n",
      "Epoch 700/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2293 - val_loss: 0.3884\n",
      "Epoch 701/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2298 - val_loss: 0.4557\n",
      "Epoch 702/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2280 - val_loss: 0.4498\n",
      "Epoch 703/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2306 - val_loss: 0.4143\n",
      "Epoch 704/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2509 - val_loss: 0.4137\n",
      "Epoch 705/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2255 - val_loss: 0.4162\n",
      "Epoch 706/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2896 - val_loss: 0.4390\n",
      "Epoch 707/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2589 - val_loss: 0.4138\n",
      "Epoch 708/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2521 - val_loss: 0.4403\n",
      "Epoch 709/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2796 - val_loss: 0.4368\n",
      "Epoch 710/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2514 - val_loss: 0.4567\n",
      "Epoch 711/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2734 - val_loss: 0.4107\n",
      "Epoch 712/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2459 - val_loss: 0.3871\n",
      "Epoch 713/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2522 - val_loss: 0.4303\n",
      "Epoch 714/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2302 - val_loss: 0.4114\n",
      "Epoch 715/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2205 - val_loss: 0.4089\n",
      "Epoch 716/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2305 - val_loss: 0.3983\n",
      "Epoch 717/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2491 - val_loss: 0.4668\n",
      "Epoch 718/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2964 - val_loss: 0.4629\n",
      "Epoch 719/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2741 - val_loss: 0.4408\n",
      "Epoch 720/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2719 - val_loss: 0.4210\n",
      "Epoch 721/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2302 - val_loss: 0.3822\n",
      "Epoch 722/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2425 - val_loss: 0.4468\n",
      "Epoch 723/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2877 - val_loss: 0.4416\n",
      "Epoch 724/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3012 - val_loss: 0.4802\n",
      "Epoch 725/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3063 - val_loss: 0.4207\n",
      "Epoch 726/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2561 - val_loss: 0.4086\n",
      "Epoch 727/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2531 - val_loss: 0.4129\n",
      "Epoch 728/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2283 - val_loss: 0.3948\n",
      "Epoch 729/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2551 - val_loss: 0.4229\n",
      "Epoch 730/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2499 - val_loss: 0.4907\n",
      "Epoch 731/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2565 - val_loss: 0.4095\n",
      "Epoch 732/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2283 - val_loss: 0.4332\n",
      "Epoch 733/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2442 - val_loss: 0.3955\n",
      "Epoch 734/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2203 - val_loss: 0.4047\n",
      "Epoch 735/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.2255 - val_loss: 0.4276\n",
      "Epoch 736/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2208 - val_loss: 0.3876\n",
      "Epoch 737/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2375 - val_loss: 0.4107\n",
      "Epoch 738/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2618 - val_loss: 0.4029\n",
      "Epoch 739/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2158 - val_loss: 0.3964\n",
      "Epoch 740/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2423 - val_loss: 0.4081\n",
      "Epoch 741/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2820 - val_loss: 0.4857\n",
      "Epoch 742/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2955 - val_loss: 0.4542\n",
      "Epoch 743/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2848 - val_loss: 0.4049\n",
      "Epoch 744/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2271 - val_loss: 0.4054\n",
      "Epoch 745/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2253 - val_loss: 0.3956\n",
      "Epoch 746/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2618 - val_loss: 0.4377\n",
      "Epoch 747/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2586 - val_loss: 0.4231\n",
      "Epoch 748/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2573 - val_loss: 0.3807\n",
      "Epoch 749/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2364 - val_loss: 0.4023\n",
      "Epoch 750/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2254 - val_loss: 0.3895\n",
      "Epoch 751/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2587 - val_loss: 0.3719\n",
      "Epoch 752/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.2551 - val_loss: 0.4424\n",
      "Epoch 753/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2581 - val_loss: 0.4124\n",
      "Epoch 754/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2392 - val_loss: 0.4160\n",
      "Epoch 755/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2523 - val_loss: 0.4008\n",
      "Epoch 756/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2819 - val_loss: 0.3983\n",
      "Epoch 757/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2444 - val_loss: 0.4948\n",
      "Epoch 758/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2625 - val_loss: 0.4306\n",
      "Epoch 759/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2512 - val_loss: 0.4197\n",
      "Epoch 760/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2941 - val_loss: 0.4478\n",
      "Epoch 761/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2449 - val_loss: 0.3946\n",
      "Epoch 762/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2376 - val_loss: 0.4344\n",
      "Epoch 763/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2184 - val_loss: 0.3996\n",
      "Epoch 764/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2337 - val_loss: 0.3914\n",
      "Epoch 765/2000\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.220 - 1s 11ms/step - loss: 0.2214 - val_loss: 0.4111\n",
      "Epoch 766/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2567 - val_loss: 0.4259\n",
      "Epoch 767/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2280 - val_loss: 0.4564\n",
      "Epoch 768/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2302 - val_loss: 0.4134\n",
      "Epoch 769/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2261 - val_loss: 0.3985\n",
      "Epoch 770/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2261 - val_loss: 0.3966\n",
      "Epoch 771/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2567 - val_loss: 0.4519\n",
      "Epoch 772/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2592 - val_loss: 0.4424\n",
      "Epoch 773/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2972 - val_loss: 0.3903\n",
      "Epoch 774/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2379 - val_loss: 0.3985\n",
      "Epoch 775/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2236 - val_loss: 0.3979\n",
      "Epoch 776/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2355 - val_loss: 0.4307\n",
      "Epoch 777/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2581 - val_loss: 0.4253\n",
      "Epoch 778/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2753 - val_loss: 0.5004\n",
      "Epoch 779/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3814 - val_loss: 0.4160\n",
      "Epoch 780/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3011 - val_loss: 0.4151\n",
      "Epoch 781/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2659 - val_loss: 0.4156\n",
      "Epoch 782/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2450 - val_loss: 0.4195\n",
      "Epoch 783/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2401 - val_loss: 0.3879\n",
      "Epoch 784/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2440 - val_loss: 0.4227\n",
      "Epoch 785/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2357 - val_loss: 0.4247\n",
      "Epoch 786/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2372 - val_loss: 0.4200\n",
      "Epoch 787/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2655 - val_loss: 0.3968\n",
      "Epoch 788/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2304 - val_loss: 0.4025\n",
      "Epoch 789/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2241 - val_loss: 0.3986\n",
      "Epoch 790/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2281 - val_loss: 0.3939\n",
      "Epoch 791/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2331 - val_loss: 0.4003\n",
      "Epoch 792/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2339 - val_loss: 0.4280\n",
      "Epoch 793/2000\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.3114 - val_loss: 0.3939\n",
      "Epoch 794/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2428 - val_loss: 0.4116\n",
      "Epoch 795/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.2400 - val_loss: 0.3885\n",
      "Epoch 796/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2243 - val_loss: 0.3843\n",
      "Epoch 797/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2503 - val_loss: 0.4630\n",
      "Epoch 798/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2879 - val_loss: 0.4479\n",
      "Epoch 799/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.2758 - val_loss: 0.3891\n",
      "Epoch 800/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2278 - val_loss: 0.4233\n",
      "Epoch 801/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2248 - val_loss: 0.5132\n",
      "Epoch 802/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2458 - val_loss: 0.4403\n",
      "Epoch 803/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2228 - val_loss: 0.4267\n",
      "Epoch 804/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2380 - val_loss: 0.4437\n",
      "Epoch 805/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2526 - val_loss: 0.4291\n",
      "Epoch 806/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2643 - val_loss: 0.4287\n",
      "Epoch 807/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3077 - val_loss: 0.3953\n",
      "Epoch 808/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2447 - val_loss: 0.3894\n",
      "Epoch 809/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2563 - val_loss: 0.4077\n",
      "Epoch 810/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2217 - val_loss: 0.4195\n",
      "Epoch 811/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2427 - val_loss: 0.4183\n",
      "Epoch 812/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2444 - val_loss: 0.4165\n",
      "Epoch 813/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2253 - val_loss: 0.3940\n",
      "Epoch 814/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2150 - val_loss: 0.3824\n",
      "Epoch 815/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2229 - val_loss: 0.3848\n",
      "Epoch 816/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2191 - val_loss: 0.4292\n",
      "Epoch 817/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2227 - val_loss: 0.4431\n",
      "Epoch 818/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2214 - val_loss: 0.4263\n",
      "Epoch 819/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2401 - val_loss: 0.4727\n",
      "Epoch 820/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2969 - val_loss: 0.4324\n",
      "Epoch 821/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2255 - val_loss: 0.3891\n",
      "Epoch 822/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2247 - val_loss: 0.4531\n",
      "Epoch 823/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2464 - val_loss: 0.3967\n",
      "Epoch 824/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2300 - val_loss: 0.3950\n",
      "Epoch 825/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2325 - val_loss: 0.4106\n",
      "Epoch 826/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2325 - val_loss: 0.4009\n",
      "Epoch 827/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2353 - val_loss: 0.4102\n",
      "Epoch 828/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2263 - val_loss: 0.4470\n",
      "Epoch 829/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2834 - val_loss: 0.3935\n",
      "Epoch 830/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2585 - val_loss: 0.4231\n",
      "Epoch 831/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2339 - val_loss: 0.4275\n",
      "Epoch 832/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2364 - val_loss: 0.4015\n",
      "Epoch 833/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2168 - val_loss: 0.3849\n",
      "Epoch 834/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2232 - val_loss: 0.3885\n",
      "Epoch 835/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2325 - val_loss: 0.4858\n",
      "Epoch 836/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2694 - val_loss: 0.4623\n",
      "Epoch 837/2000\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.2217 - val_loss: 0.3999\n",
      "Epoch 838/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2228 - val_loss: 0.4122\n",
      "Epoch 839/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2750 - val_loss: 0.4156\n",
      "Epoch 840/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2521 - val_loss: 0.4444\n",
      "Epoch 841/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2387 - val_loss: 0.3983\n",
      "Epoch 842/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2566 - val_loss: 0.4253\n",
      "Epoch 843/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2264 - val_loss: 0.4127\n",
      "Epoch 844/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2381 - val_loss: 0.3865\n",
      "Epoch 845/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2256 - val_loss: 0.4098\n",
      "Epoch 846/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2318 - val_loss: 0.3962\n",
      "Epoch 847/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2351 - val_loss: 0.4467\n",
      "Epoch 848/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2549 - val_loss: 0.4914\n",
      "Epoch 849/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2678 - val_loss: 0.4045\n",
      "Epoch 850/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2201 - val_loss: 0.3876\n",
      "Epoch 851/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2301 - val_loss: 0.4032\n",
      "Epoch 852/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2099 - val_loss: 0.4072\n",
      "Epoch 853/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2172 - val_loss: 0.3981\n",
      "Epoch 854/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2129 - val_loss: 0.4094\n",
      "Epoch 855/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2145 - val_loss: 0.4235\n",
      "Epoch 856/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2625 - val_loss: 0.4786\n",
      "Epoch 857/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2455 - val_loss: 0.4876\n",
      "Epoch 858/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2487 - val_loss: 0.4239\n",
      "Epoch 859/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2450 - val_loss: 0.5049\n",
      "Epoch 860/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2632 - val_loss: 0.4002\n",
      "Epoch 861/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2278 - val_loss: 0.3953\n",
      "Epoch 862/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2316 - val_loss: 0.4544\n",
      "Epoch 863/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2263 - val_loss: 0.3812\n",
      "Epoch 864/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2230 - val_loss: 0.4091\n",
      "Epoch 865/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2703 - val_loss: 0.4129\n",
      "Epoch 866/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2427 - val_loss: 0.4124\n",
      "Epoch 867/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2300 - val_loss: 0.4692\n",
      "Epoch 868/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2046 - val_loss: 0.4009\n",
      "Epoch 869/2000\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.2291 - val_loss: 0.4224\n",
      "Epoch 870/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2332 - val_loss: 0.3922\n",
      "Epoch 871/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2155 - val_loss: 0.4488\n",
      "Epoch 872/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2454 - val_loss: 0.4246\n",
      "Epoch 873/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2374 - val_loss: 0.4040\n",
      "Epoch 874/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2301 - val_loss: 0.3887\n",
      "Epoch 875/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2195 - val_loss: 0.4074\n",
      "Epoch 876/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2464 - val_loss: 0.4238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 877/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2323 - val_loss: 0.4370\n",
      "Epoch 878/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2126 - val_loss: 0.4140\n",
      "Epoch 879/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2203 - val_loss: 0.4441\n",
      "Epoch 880/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2574 - val_loss: 0.4390\n",
      "Epoch 881/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2388 - val_loss: 0.4412\n",
      "Epoch 882/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2205 - val_loss: 0.3969\n",
      "Epoch 883/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2071 - val_loss: 0.4181\n",
      "Epoch 884/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2118 - val_loss: 0.3971\n",
      "Epoch 885/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2277 - val_loss: 0.3970\n",
      "Epoch 886/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2620 - val_loss: 0.3831\n",
      "Epoch 887/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2410 - val_loss: 0.4126\n",
      "Epoch 888/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2526 - val_loss: 0.4233\n",
      "Epoch 889/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2188 - val_loss: 0.4126\n",
      "Epoch 890/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2259 - val_loss: 0.4515\n",
      "Epoch 891/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2097 - val_loss: 0.3996\n",
      "Epoch 892/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2175 - val_loss: 0.4224\n",
      "Epoch 893/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2257 - val_loss: 0.4157\n",
      "Epoch 894/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2170 - val_loss: 0.4296\n",
      "Epoch 895/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2193 - val_loss: 0.3834\n",
      "Epoch 896/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.2176 - val_loss: 0.4105\n",
      "Epoch 897/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2415 - val_loss: 0.4240\n",
      "Epoch 898/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2243 - val_loss: 0.4029\n",
      "Epoch 899/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1945 - val_loss: 0.4299\n",
      "Epoch 900/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2001 - val_loss: 0.3861\n",
      "Epoch 901/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2203 - val_loss: 0.3930\n",
      "Epoch 902/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2341 - val_loss: 0.4730\n",
      "Epoch 903/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2511 - val_loss: 0.4305\n",
      "Epoch 904/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2889 - val_loss: 0.4401\n",
      "Epoch 905/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2467 - val_loss: 0.3847\n",
      "Epoch 906/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2168 - val_loss: 0.4041\n",
      "Epoch 907/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2166 - val_loss: 0.4330\n",
      "Epoch 908/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2565 - val_loss: 0.5184\n",
      "Epoch 909/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2984 - val_loss: 0.4480\n",
      "Epoch 910/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2317 - val_loss: 0.4169\n",
      "Epoch 911/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2156 - val_loss: 0.4171\n",
      "Epoch 912/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1993 - val_loss: 0.3889\n",
      "Epoch 913/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.2061 - val_loss: 0.4041\n",
      "Epoch 914/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2194 - val_loss: 0.4320\n",
      "Epoch 915/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2341 - val_loss: 0.4290\n",
      "Epoch 916/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2183 - val_loss: 0.3879\n",
      "Epoch 917/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2072 - val_loss: 0.4062\n",
      "Epoch 918/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2171 - val_loss: 0.3876\n",
      "Epoch 919/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1971 - val_loss: 0.3980\n",
      "Epoch 920/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2346 - val_loss: 0.3968\n",
      "Epoch 921/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2704 - val_loss: 0.4135\n",
      "Epoch 922/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2374 - val_loss: 0.3925\n",
      "Epoch 923/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2140 - val_loss: 0.4518\n",
      "Epoch 924/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2252 - val_loss: 0.4068\n",
      "Epoch 925/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2314 - val_loss: 0.4208\n",
      "Epoch 926/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.2245 - val_loss: 0.4045\n",
      "Epoch 927/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2149 - val_loss: 0.4412\n",
      "Epoch 928/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2231 - val_loss: 0.4175\n",
      "Epoch 929/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2049 - val_loss: 0.3975\n",
      "Epoch 930/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2525 - val_loss: 0.4448\n",
      "Epoch 931/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2266 - val_loss: 0.4078\n",
      "Epoch 932/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1959 - val_loss: 0.3989\n",
      "Epoch 933/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2059 - val_loss: 0.4652\n",
      "Epoch 934/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2834 - val_loss: 0.4423\n",
      "Epoch 935/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2378 - val_loss: 0.4273\n",
      "Epoch 936/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2510 - val_loss: 0.4139\n",
      "Epoch 937/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2093 - val_loss: 0.4059\n",
      "Epoch 938/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2113 - val_loss: 0.3906\n",
      "Epoch 939/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1930 - val_loss: 0.3801\n",
      "Epoch 940/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2127 - val_loss: 0.4695\n",
      "Epoch 941/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2209 - val_loss: 0.4162\n",
      "Epoch 942/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2310 - val_loss: 0.3906\n",
      "Epoch 943/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2302 - val_loss: 0.3979\n",
      "Epoch 944/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2194 - val_loss: 0.4322\n",
      "Epoch 945/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2436 - val_loss: 0.4129\n",
      "Epoch 946/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2071 - val_loss: 0.4164\n",
      "Epoch 947/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2363 - val_loss: 0.4353\n",
      "Epoch 948/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.3014 - val_loss: 0.4409\n",
      "Epoch 949/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2548 - val_loss: 0.3853\n",
      "Epoch 950/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2209 - val_loss: 0.4028\n",
      "Epoch 951/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2065 - val_loss: 0.4249\n",
      "Epoch 952/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2314 - val_loss: 0.3964\n",
      "Epoch 953/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2071 - val_loss: 0.4169\n",
      "Epoch 954/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2026 - val_loss: 0.4112\n",
      "Epoch 955/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1996 - val_loss: 0.3759\n",
      "Epoch 956/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1910 - val_loss: 0.4201\n",
      "Epoch 957/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2465 - val_loss: 0.4230\n",
      "Epoch 958/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2215 - val_loss: 0.3926\n",
      "Epoch 959/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2012 - val_loss: 0.4062\n",
      "Epoch 960/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1961 - val_loss: 0.3968\n",
      "Epoch 961/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1955 - val_loss: 0.3923\n",
      "Epoch 962/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2162 - val_loss: 0.4104\n",
      "Epoch 963/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1959 - val_loss: 0.4193\n",
      "Epoch 964/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2525 - val_loss: 0.4285\n",
      "Epoch 965/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2224 - val_loss: 0.3880\n",
      "Epoch 966/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2081 - val_loss: 0.4002\n",
      "Epoch 967/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2259 - val_loss: 0.4664\n",
      "Epoch 968/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3886 - val_loss: 0.4317\n",
      "Epoch 969/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2630 - val_loss: 0.4212\n",
      "Epoch 970/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2493 - val_loss: 0.4208\n",
      "Epoch 971/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2174 - val_loss: 0.4146\n",
      "Epoch 972/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2180 - val_loss: 0.4081\n",
      "Epoch 973/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2065 - val_loss: 0.3909\n",
      "Epoch 974/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2227 - val_loss: 0.3850\n",
      "Epoch 975/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2105 - val_loss: 0.3976\n",
      "Epoch 976/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2057 - val_loss: 0.3907\n",
      "Epoch 977/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1956 - val_loss: 0.3953\n",
      "Epoch 978/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2305 - val_loss: 0.4184\n",
      "Epoch 979/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1994 - val_loss: 0.3897\n",
      "Epoch 980/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2065 - val_loss: 0.4011\n",
      "Epoch 981/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2088 - val_loss: 0.3900\n",
      "Epoch 982/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2340 - val_loss: 0.4025\n",
      "Epoch 983/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1938 - val_loss: 0.3904\n",
      "Epoch 984/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2049 - val_loss: 0.4418\n",
      "Epoch 985/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2491 - val_loss: 0.3889\n",
      "Epoch 986/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2232 - val_loss: 0.4344\n",
      "Epoch 987/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2157 - val_loss: 0.3889\n",
      "Epoch 988/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2092 - val_loss: 0.4184\n",
      "Epoch 989/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1990 - val_loss: 0.4179\n",
      "Epoch 990/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2122 - val_loss: 0.3800\n",
      "Epoch 991/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.1949 - val_loss: 0.4089\n",
      "Epoch 992/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2472 - val_loss: 0.4461\n",
      "Epoch 993/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2302 - val_loss: 0.3950\n",
      "Epoch 994/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2014 - val_loss: 0.3754\n",
      "Epoch 995/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2079 - val_loss: 0.3909\n",
      "Epoch 996/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2317 - val_loss: 0.3973\n",
      "Epoch 997/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.2177 - val_loss: 0.4066\n",
      "Epoch 998/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1994 - val_loss: 0.4233\n",
      "Epoch 999/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.2214 - val_loss: 0.4252\n",
      "Epoch 1000/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2267 - val_loss: 0.3840\n",
      "Epoch 1001/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2523 - val_loss: 0.4244\n",
      "Epoch 1002/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2838 - val_loss: 0.4500\n",
      "Epoch 1003/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2195 - val_loss: 0.4165\n",
      "Epoch 1004/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1922 - val_loss: 0.3816\n",
      "Epoch 1005/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1897 - val_loss: 0.4108\n",
      "Epoch 1006/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2241 - val_loss: 0.4097\n",
      "Epoch 1007/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2026 - val_loss: 0.4089\n",
      "Epoch 1008/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2381 - val_loss: 0.4426\n",
      "Epoch 1009/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2067 - val_loss: 0.3900\n",
      "Epoch 1010/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2301 - val_loss: 0.4299\n",
      "Epoch 1011/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2195 - val_loss: 0.4062\n",
      "Epoch 1012/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1979 - val_loss: 0.3963\n",
      "Epoch 1013/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1995 - val_loss: 0.3906\n",
      "Epoch 1014/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2007 - val_loss: 0.4546\n",
      "Epoch 1015/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2467 - val_loss: 0.4230\n",
      "Epoch 1016/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2231 - val_loss: 0.4081\n",
      "Epoch 1017/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2061 - val_loss: 0.3862\n",
      "Epoch 1018/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1844 - val_loss: 0.3858\n",
      "Epoch 1019/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2019 - val_loss: 0.4007\n",
      "Epoch 1020/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2001 - val_loss: 0.4153\n",
      "Epoch 1021/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2378 - val_loss: 0.4537\n",
      "Epoch 1022/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2683 - val_loss: 0.4835\n",
      "Epoch 1023/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2273 - val_loss: 0.3916\n",
      "Epoch 1024/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2013 - val_loss: 0.4127\n",
      "Epoch 1025/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1944 - val_loss: 0.3951\n",
      "Epoch 1026/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2087 - val_loss: 0.3998\n",
      "Epoch 1027/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2002 - val_loss: 0.4035\n",
      "Epoch 1028/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2122 - val_loss: 0.4137\n",
      "Epoch 1029/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2026 - val_loss: 0.4099\n",
      "Epoch 1030/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1988 - val_loss: 0.4332\n",
      "Epoch 1031/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2145 - val_loss: 0.4265\n",
      "Epoch 1032/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2037 - val_loss: 0.4177\n",
      "Epoch 1033/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2385 - val_loss: 0.4482\n",
      "Epoch 1034/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2127 - val_loss: 0.4161\n",
      "Epoch 1035/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2098 - val_loss: 0.4303\n",
      "Epoch 1036/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2218 - val_loss: 0.4378\n",
      "Epoch 1037/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2364 - val_loss: 0.4113\n",
      "Epoch 1038/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2187 - val_loss: 0.3956\n",
      "Epoch 1039/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2078 - val_loss: 0.4043\n",
      "Epoch 1040/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2013 - val_loss: 0.4178\n",
      "Epoch 1041/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2595 - val_loss: 0.4785\n",
      "Epoch 1042/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2367 - val_loss: 0.4305\n",
      "Epoch 1043/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2701 - val_loss: 0.4066\n",
      "Epoch 1044/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2002 - val_loss: 0.4459\n",
      "Epoch 1045/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1973 - val_loss: 0.3835\n",
      "Epoch 1046/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2012 - val_loss: 0.4193\n",
      "Epoch 1047/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2146 - val_loss: 0.4176\n",
      "Epoch 1048/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1957 - val_loss: 0.4274\n",
      "Epoch 1049/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2009 - val_loss: 0.4143\n",
      "Epoch 1050/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2522 - val_loss: 0.4170\n",
      "Epoch 1051/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2145 - val_loss: 0.4151\n",
      "Epoch 1052/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2007 - val_loss: 0.4171\n",
      "Epoch 1053/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2025 - val_loss: 0.3920\n",
      "Epoch 1054/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2001 - val_loss: 0.4363\n",
      "Epoch 1055/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2236 - val_loss: 0.4370\n",
      "Epoch 1056/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2201 - val_loss: 0.4130\n",
      "Epoch 1057/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1844 - val_loss: 0.4017\n",
      "Epoch 1058/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1892 - val_loss: 0.4279\n",
      "Epoch 1059/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1976 - val_loss: 0.3909\n",
      "Epoch 1060/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2426 - val_loss: 0.4295\n",
      "Epoch 1061/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2167 - val_loss: 0.4053\n",
      "Epoch 1062/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2042 - val_loss: 0.4271\n",
      "Epoch 1063/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.2137 - val_loss: 0.3997\n",
      "Epoch 1064/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1952 - val_loss: 0.3952\n",
      "Epoch 1065/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1966 - val_loss: 0.3898\n",
      "Epoch 1066/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1897 - val_loss: 0.4148\n",
      "Epoch 1067/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2150 - val_loss: 0.3883\n",
      "Epoch 1068/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1951 - val_loss: 0.3878\n",
      "Epoch 1069/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1910 - val_loss: 0.3879\n",
      "Epoch 1070/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2070 - val_loss: 0.3953\n",
      "Epoch 1071/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2116 - val_loss: 0.3825\n",
      "Epoch 1072/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1977 - val_loss: 0.4060\n",
      "Epoch 1073/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2268 - val_loss: 0.4089\n",
      "Epoch 1074/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2051 - val_loss: 0.4134\n",
      "Epoch 1075/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1892 - val_loss: 0.4058\n",
      "Epoch 1076/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2118 - val_loss: 0.4170\n",
      "Epoch 1077/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2005 - val_loss: 0.3935\n",
      "Epoch 1078/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1992 - val_loss: 0.4061\n",
      "Epoch 1079/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2441 - val_loss: 0.4139\n",
      "Epoch 1080/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2200 - val_loss: 0.3976\n",
      "Epoch 1081/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1909 - val_loss: 0.3923\n",
      "Epoch 1082/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1797 - val_loss: 0.3811\n",
      "Epoch 1083/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1893 - val_loss: 0.4018\n",
      "Epoch 1084/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1852 - val_loss: 0.3981\n",
      "Epoch 1085/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2178 - val_loss: 0.3954\n",
      "Epoch 1086/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2222 - val_loss: 0.4147\n",
      "Epoch 1087/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2079 - val_loss: 0.3827\n",
      "Epoch 1088/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.2275 - val_loss: 0.4706\n",
      "Epoch 1089/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3110 - val_loss: 0.5050\n",
      "Epoch 1090/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2479 - val_loss: 0.4208\n",
      "Epoch 1091/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2220 - val_loss: 0.4167\n",
      "Epoch 1092/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2216 - val_loss: 0.4369\n",
      "Epoch 1093/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1942 - val_loss: 0.3783\n",
      "Epoch 1094/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2001 - val_loss: 0.4396\n",
      "Epoch 1095/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2159 - val_loss: 0.3869\n",
      "Epoch 1096/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1848 - val_loss: 0.4039\n",
      "Epoch 1097/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1998 - val_loss: 0.3983\n",
      "Epoch 1098/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1882 - val_loss: 0.3842\n",
      "Epoch 1099/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2016 - val_loss: 0.4119\n",
      "Epoch 1100/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2509 - val_loss: 0.4147\n",
      "Epoch 1101/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2259 - val_loss: 0.3978\n",
      "Epoch 1102/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2283 - val_loss: 0.4240\n",
      "Epoch 1103/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2124 - val_loss: 0.3854\n",
      "Epoch 1104/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2429 - val_loss: 0.4196\n",
      "Epoch 1105/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2053 - val_loss: 0.4005\n",
      "Epoch 1106/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1952 - val_loss: 0.4037\n",
      "Epoch 1107/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2444 - val_loss: 0.3834\n",
      "Epoch 1108/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1861 - val_loss: 0.4113\n",
      "Epoch 1109/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1864 - val_loss: 0.3955\n",
      "Epoch 1110/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2260 - val_loss: 0.3821\n",
      "Epoch 1111/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2047 - val_loss: 0.3868\n",
      "Epoch 1112/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.2090 - val_loss: 0.4097\n",
      "Epoch 1113/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2630 - val_loss: 0.4444\n",
      "Epoch 1114/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2143 - val_loss: 0.4319\n",
      "Epoch 1115/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1991 - val_loss: 0.3969\n",
      "Epoch 1116/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1928 - val_loss: 0.4269\n",
      "Epoch 1117/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1896 - val_loss: 0.3855\n",
      "Epoch 1118/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1943 - val_loss: 0.4021\n",
      "Epoch 1119/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2121 - val_loss: 0.3960\n",
      "Epoch 1120/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2084 - val_loss: 0.3901\n",
      "Epoch 1121/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2326 - val_loss: 0.4233\n",
      "Epoch 1122/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2247 - val_loss: 0.4078\n",
      "Epoch 1123/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1995 - val_loss: 0.4222\n",
      "Epoch 1124/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2107 - val_loss: 0.3887\n",
      "Epoch 1125/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1955 - val_loss: 0.3953\n",
      "Epoch 1126/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1898 - val_loss: 0.3897\n",
      "Epoch 1127/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1787 - val_loss: 0.3760\n",
      "Epoch 1128/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1943 - val_loss: 0.4309\n",
      "Epoch 1129/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2240 - val_loss: 0.3925\n",
      "Epoch 1130/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2047 - val_loss: 0.4061\n",
      "Epoch 1131/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1828 - val_loss: 0.3904\n",
      "Epoch 1132/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2106 - val_loss: 0.4227\n",
      "Epoch 1133/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2334 - val_loss: 0.3901\n",
      "Epoch 1134/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2165 - val_loss: 0.4360\n",
      "Epoch 1135/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1913 - val_loss: 0.3960\n",
      "Epoch 1136/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2201 - val_loss: 0.4510\n",
      "Epoch 1137/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3027 - val_loss: 0.4584\n",
      "Epoch 1138/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2133 - val_loss: 0.4202\n",
      "Epoch 1139/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2227 - val_loss: 0.4046\n",
      "Epoch 1140/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2049 - val_loss: 0.4053\n",
      "Epoch 1141/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1902 - val_loss: 0.3859\n",
      "Epoch 1142/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1878 - val_loss: 0.4023\n",
      "Epoch 1143/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1919 - val_loss: 0.4159\n",
      "Epoch 1144/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1898 - val_loss: 0.3847\n",
      "Epoch 1145/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1888 - val_loss: 0.4069\n",
      "Epoch 1146/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1828 - val_loss: 0.4564\n",
      "Epoch 1147/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3981 - val_loss: 0.4440\n",
      "Epoch 1148/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2441 - val_loss: 0.4145\n",
      "Epoch 1149/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2137 - val_loss: 0.3885\n",
      "Epoch 1150/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1929 - val_loss: 0.3799\n",
      "Epoch 1151/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1827 - val_loss: 0.3972\n",
      "Epoch 1152/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1887 - val_loss: 0.3989\n",
      "Epoch 1153/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1988 - val_loss: 0.4193\n",
      "Epoch 1154/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1795 - val_loss: 0.3842\n",
      "Epoch 1155/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2129 - val_loss: 0.3992\n",
      "Epoch 1156/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1926 - val_loss: 0.4114\n",
      "Epoch 1157/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1871 - val_loss: 0.4122\n",
      "Epoch 1158/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1844 - val_loss: 0.3950\n",
      "Epoch 1159/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1891 - val_loss: 0.3874\n",
      "Epoch 1160/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1847 - val_loss: 0.3761\n",
      "Epoch 1161/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1895 - val_loss: 0.3940\n",
      "Epoch 1162/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2161 - val_loss: 0.3860\n",
      "Epoch 1163/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2116 - val_loss: 0.3910\n",
      "Epoch 1164/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2044 - val_loss: 0.4109\n",
      "Epoch 1165/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2199 - val_loss: 0.3817\n",
      "Epoch 1166/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1910 - val_loss: 0.3899\n",
      "Epoch 1167/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1835 - val_loss: 0.3823\n",
      "Epoch 1168/2000\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.1893 - val_loss: 0.3946\n",
      "Epoch 1169/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2090 - val_loss: 0.4416\n",
      "Epoch 1170/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2235 - val_loss: 0.4430\n",
      "Epoch 1171/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2543 - val_loss: 0.4079\n",
      "Epoch 1172/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2590 - val_loss: 0.3836\n",
      "Epoch 1173/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1977 - val_loss: 0.3640\n",
      "Epoch 1174/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1825 - val_loss: 0.3817\n",
      "Epoch 1175/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1896 - val_loss: 0.3779\n",
      "Epoch 1176/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1957 - val_loss: 0.3970\n",
      "Epoch 1177/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1880 - val_loss: 0.4177\n",
      "Epoch 1178/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2244 - val_loss: 0.3936\n",
      "Epoch 1179/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1912 - val_loss: 0.4210\n",
      "Epoch 1180/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1872 - val_loss: 0.3816\n",
      "Epoch 1181/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1801 - val_loss: 0.4025\n",
      "Epoch 1182/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1850 - val_loss: 0.4096\n",
      "Epoch 1183/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1883 - val_loss: 0.3881\n",
      "Epoch 1184/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.2012 - val_loss: 0.4014\n",
      "Epoch 1185/2000\n",
      "80/80 [==============================] - 2s 19ms/step - loss: 0.1870 - val_loss: 0.4050\n",
      "Epoch 1186/2000\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.2046 - val_loss: 0.3977\n",
      "Epoch 1187/2000\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.2548 - val_loss: 0.4413\n",
      "Epoch 1188/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2121 - val_loss: 0.4136\n",
      "Epoch 1189/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1850 - val_loss: 0.3934\n",
      "Epoch 1190/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1812 - val_loss: 0.3916\n",
      "Epoch 1191/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1920 - val_loss: 0.3811\n",
      "Epoch 1192/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1834 - val_loss: 0.3823\n",
      "Epoch 1193/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1738 - val_loss: 0.4112\n",
      "Epoch 1194/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1864 - val_loss: 0.4031\n",
      "Epoch 1195/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2163 - val_loss: 0.4473\n",
      "Epoch 1196/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2752 - val_loss: 0.3929\n",
      "Epoch 1197/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2405 - val_loss: 0.4212\n",
      "Epoch 1198/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1990 - val_loss: 0.3926\n",
      "Epoch 1199/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1963 - val_loss: 0.4437\n",
      "Epoch 1200/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2362 - val_loss: 0.4223\n",
      "Epoch 1201/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1887 - val_loss: 0.4114\n",
      "Epoch 1202/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1879 - val_loss: 0.3860\n",
      "Epoch 1203/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1864 - val_loss: 0.3850\n",
      "Epoch 1204/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1895 - val_loss: 0.4223\n",
      "Epoch 1205/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1831 - val_loss: 0.4079\n",
      "Epoch 1206/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1789 - val_loss: 0.3791\n",
      "Epoch 1207/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2001 - val_loss: 0.3857\n",
      "Epoch 1208/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1823 - val_loss: 0.4020\n",
      "Epoch 1209/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1909 - val_loss: 0.3999\n",
      "Epoch 1210/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1842 - val_loss: 0.3710\n",
      "Epoch 1211/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1846 - val_loss: 0.4128\n",
      "Epoch 1212/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2179 - val_loss: 0.3986\n",
      "Epoch 1213/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1826 - val_loss: 0.3823\n",
      "Epoch 1214/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1730 - val_loss: 0.3766\n",
      "Epoch 1215/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1949 - val_loss: 0.4448\n",
      "Epoch 1216/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1867 - val_loss: 0.3940\n",
      "Epoch 1217/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1728 - val_loss: 0.3711\n",
      "Epoch 1218/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1823 - val_loss: 0.3857\n",
      "Epoch 1219/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1969 - val_loss: 0.4652\n",
      "Epoch 1220/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2151 - val_loss: 0.4040\n",
      "Epoch 1221/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1901 - val_loss: 0.3856\n",
      "Epoch 1222/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1858 - val_loss: 0.4134\n",
      "Epoch 1223/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2268 - val_loss: 0.4118\n",
      "Epoch 1224/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1866 - val_loss: 0.4021\n",
      "Epoch 1225/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1863 - val_loss: 0.3843\n",
      "Epoch 1226/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1918 - val_loss: 0.3835\n",
      "Epoch 1227/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1849 - val_loss: 0.3875\n",
      "Epoch 1228/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2086 - val_loss: 0.4101\n",
      "Epoch 1229/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1996 - val_loss: 0.4098\n",
      "Epoch 1230/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1936 - val_loss: 0.3909\n",
      "Epoch 1231/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1867 - val_loss: 0.3828\n",
      "Epoch 1232/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2021 - val_loss: 0.4679\n",
      "Epoch 1233/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2199 - val_loss: 0.4142\n",
      "Epoch 1234/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1958 - val_loss: 0.3868\n",
      "Epoch 1235/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1926 - val_loss: 0.4406\n",
      "Epoch 1236/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1952 - val_loss: 0.3847\n",
      "Epoch 1237/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1787 - val_loss: 0.3929\n",
      "Epoch 1238/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1936 - val_loss: 0.4371\n",
      "Epoch 1239/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2207 - val_loss: 0.3902\n",
      "Epoch 1240/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1997 - val_loss: 0.5194\n",
      "Epoch 1241/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2308 - val_loss: 0.4195\n",
      "Epoch 1242/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1860 - val_loss: 0.3878\n",
      "Epoch 1243/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1757 - val_loss: 0.4369\n",
      "Epoch 1244/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2043 - val_loss: 0.4472\n",
      "Epoch 1245/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2114 - val_loss: 0.3959\n",
      "Epoch 1246/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1936 - val_loss: 0.4147\n",
      "Epoch 1247/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1985 - val_loss: 0.4184\n",
      "Epoch 1248/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1969 - val_loss: 0.4126\n",
      "Epoch 1249/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1841 - val_loss: 0.5421\n",
      "Epoch 1250/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2492 - val_loss: 0.3967\n",
      "Epoch 1251/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1979 - val_loss: 0.4196\n",
      "Epoch 1252/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2060 - val_loss: 0.5378\n",
      "Epoch 1253/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2271 - val_loss: 0.4022\n",
      "Epoch 1254/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1833 - val_loss: 0.4119\n",
      "Epoch 1255/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1813 - val_loss: 0.4108\n",
      "Epoch 1256/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1782 - val_loss: 0.4099\n",
      "Epoch 1257/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1820 - val_loss: 0.4152\n",
      "Epoch 1258/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1779 - val_loss: 0.3817\n",
      "Epoch 1259/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1848 - val_loss: 0.3916\n",
      "Epoch 1260/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1738 - val_loss: 0.4164\n",
      "Epoch 1261/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1856 - val_loss: 0.4126\n",
      "Epoch 1262/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1883 - val_loss: 0.3872\n",
      "Epoch 1263/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1742 - val_loss: 0.3916\n",
      "Epoch 1264/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1936 - val_loss: 0.4011\n",
      "Epoch 1265/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1863 - val_loss: 0.3926\n",
      "Epoch 1266/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1808 - val_loss: 0.3949\n",
      "Epoch 1267/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1904 - val_loss: 0.4224\n",
      "Epoch 1268/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1781 - val_loss: 0.4090\n",
      "Epoch 1269/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2302 - val_loss: 0.4509\n",
      "Epoch 1270/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2765 - val_loss: 0.4137\n",
      "Epoch 1271/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2303 - val_loss: 0.4602\n",
      "Epoch 1272/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2192 - val_loss: 0.3878\n",
      "Epoch 1273/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1827 - val_loss: 0.4001\n",
      "Epoch 1274/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1816 - val_loss: 0.3893\n",
      "Epoch 1275/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1797 - val_loss: 0.3949\n",
      "Epoch 1276/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2138 - val_loss: 0.4251\n",
      "Epoch 1277/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2252 - val_loss: 0.4109\n",
      "Epoch 1278/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2053 - val_loss: 0.4046\n",
      "Epoch 1279/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1882 - val_loss: 0.3856\n",
      "Epoch 1280/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1848 - val_loss: 0.3990\n",
      "Epoch 1281/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1686 - val_loss: 0.3814\n",
      "Epoch 1282/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1723 - val_loss: 0.3980\n",
      "Epoch 1283/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1703 - val_loss: 0.3920\n",
      "Epoch 1284/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1842 - val_loss: 0.3867\n",
      "Epoch 1285/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2260 - val_loss: 0.4444\n",
      "Epoch 1286/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2070 - val_loss: 0.4017\n",
      "Epoch 1287/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1861 - val_loss: 0.4044\n",
      "Epoch 1288/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1922 - val_loss: 0.3844\n",
      "Epoch 1289/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1831 - val_loss: 0.4153\n",
      "Epoch 1290/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1813 - val_loss: 0.4026\n",
      "Epoch 1291/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1833 - val_loss: 0.3756\n",
      "Epoch 1292/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1977 - val_loss: 0.4390\n",
      "Epoch 1293/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1954 - val_loss: 0.3775\n",
      "Epoch 1294/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1759 - val_loss: 0.3939\n",
      "Epoch 1295/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1812 - val_loss: 0.4086\n",
      "Epoch 1296/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1818 - val_loss: 0.4165\n",
      "Epoch 1297/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1791 - val_loss: 0.4176\n",
      "Epoch 1298/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1717 - val_loss: 0.3982\n",
      "Epoch 1299/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1777 - val_loss: 0.4287\n",
      "Epoch 1300/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1766 - val_loss: 0.3823\n",
      "Epoch 1301/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1935 - val_loss: 0.4162\n",
      "Epoch 1302/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1826 - val_loss: 0.4209\n",
      "Epoch 1303/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2245 - val_loss: 0.4883\n",
      "Epoch 1304/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2281 - val_loss: 0.4142\n",
      "Epoch 1305/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1901 - val_loss: 0.4254\n",
      "Epoch 1306/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2234 - val_loss: 0.4348\n",
      "Epoch 1307/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1968 - val_loss: 0.4354\n",
      "Epoch 1308/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1926 - val_loss: 0.4902\n",
      "Epoch 1309/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2058 - val_loss: 0.4087\n",
      "Epoch 1310/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1696 - val_loss: 0.3963\n",
      "Epoch 1311/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1810 - val_loss: 0.4211\n",
      "Epoch 1312/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1749 - val_loss: 0.4098\n",
      "Epoch 1313/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1709 - val_loss: 0.4285\n",
      "Epoch 1314/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1813 - val_loss: 0.4000\n",
      "Epoch 1315/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1856 - val_loss: 0.3890\n",
      "Epoch 1316/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1781 - val_loss: 0.4057\n",
      "Epoch 1317/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1807 - val_loss: 0.4093\n",
      "Epoch 1318/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1853 - val_loss: 0.4057\n",
      "Epoch 1319/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1810 - val_loss: 0.4025\n",
      "Epoch 1320/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1874 - val_loss: 0.4163\n",
      "Epoch 1321/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2190 - val_loss: 0.4229\n",
      "Epoch 1322/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1845 - val_loss: 0.4051\n",
      "Epoch 1323/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2067 - val_loss: 0.4805\n",
      "Epoch 1324/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2257 - val_loss: 0.3860\n",
      "Epoch 1325/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1811 - val_loss: 0.3905\n",
      "Epoch 1326/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1883 - val_loss: 0.4165\n",
      "Epoch 1327/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1771 - val_loss: 0.4075\n",
      "Epoch 1328/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1919 - val_loss: 0.3849\n",
      "Epoch 1329/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1888 - val_loss: 0.4198\n",
      "Epoch 1330/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1945 - val_loss: 0.4119\n",
      "Epoch 1331/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1997 - val_loss: 0.4248\n",
      "Epoch 1332/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1861 - val_loss: 0.3916\n",
      "Epoch 1333/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2167 - val_loss: 0.4480\n",
      "Epoch 1334/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2117 - val_loss: 0.4133\n",
      "Epoch 1335/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1738 - val_loss: 0.4012\n",
      "Epoch 1336/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1815 - val_loss: 0.4011\n",
      "Epoch 1337/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1857 - val_loss: 0.4512\n",
      "Epoch 1338/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2368 - val_loss: 0.3993\n",
      "Epoch 1339/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1886 - val_loss: 0.3993\n",
      "Epoch 1340/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1709 - val_loss: 0.3961\n",
      "Epoch 1341/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2090 - val_loss: 0.4402\n",
      "Epoch 1342/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2385 - val_loss: 0.5452\n",
      "Epoch 1343/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2780 - val_loss: 0.5173\n",
      "Epoch 1344/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2224 - val_loss: 0.3823\n",
      "Epoch 1345/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1648 - val_loss: 0.3926\n",
      "Epoch 1346/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1805 - val_loss: 0.4423\n",
      "Epoch 1347/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1775 - val_loss: 0.3898\n",
      "Epoch 1348/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1751 - val_loss: 0.3945\n",
      "Epoch 1349/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1685 - val_loss: 0.4048\n",
      "Epoch 1350/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1818 - val_loss: 0.3950\n",
      "Epoch 1351/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1793 - val_loss: 0.4022\n",
      "Epoch 1352/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1688 - val_loss: 0.3848\n",
      "Epoch 1353/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1946 - val_loss: 0.4143\n",
      "Epoch 1354/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2089 - val_loss: 0.3929\n",
      "Epoch 1355/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1701 - val_loss: 0.3933\n",
      "Epoch 1356/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2257 - val_loss: 0.4288\n",
      "Epoch 1357/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.2139 - val_loss: 0.3775\n",
      "Epoch 1358/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1735 - val_loss: 0.3896\n",
      "Epoch 1359/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.1885 - val_loss: 0.4146\n",
      "Epoch 1360/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1801 - val_loss: 0.4032\n",
      "Epoch 1361/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1889 - val_loss: 0.4155\n",
      "Epoch 1362/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1772 - val_loss: 0.4123\n",
      "Epoch 1363/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2136 - val_loss: 0.4311\n",
      "Epoch 1364/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2150 - val_loss: 0.3763\n",
      "Epoch 1365/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1840 - val_loss: 0.3825\n",
      "Epoch 1366/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1704 - val_loss: 0.4006\n",
      "Epoch 1367/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1668 - val_loss: 0.3888\n",
      "Epoch 1368/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1712 - val_loss: 0.3949\n",
      "Epoch 1369/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1928 - val_loss: 0.4606\n",
      "Epoch 1370/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1829 - val_loss: 0.3919\n",
      "Epoch 1371/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1770 - val_loss: 0.4114\n",
      "Epoch 1372/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1993 - val_loss: 0.4242\n",
      "Epoch 1373/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1726 - val_loss: 0.4250\n",
      "Epoch 1374/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1742 - val_loss: 0.4001\n",
      "Epoch 1375/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1717 - val_loss: 0.3802\n",
      "Epoch 1376/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2288 - val_loss: 0.6835\n",
      "Epoch 1377/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2835 - val_loss: 0.3846\n",
      "Epoch 1378/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1862 - val_loss: 0.3830\n",
      "Epoch 1379/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1737 - val_loss: 0.3850\n",
      "Epoch 1380/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2013 - val_loss: 0.3811\n",
      "Epoch 1381/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1719 - val_loss: 0.4013\n",
      "Epoch 1382/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1751 - val_loss: 0.3724\n",
      "Epoch 1383/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1677 - val_loss: 0.4549\n",
      "Epoch 1384/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1852 - val_loss: 0.4025\n",
      "Epoch 1385/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1774 - val_loss: 0.4010\n",
      "Epoch 1386/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1655 - val_loss: 0.3990\n",
      "Epoch 1387/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1777 - val_loss: 0.4016\n",
      "Epoch 1388/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1658 - val_loss: 0.4173\n",
      "Epoch 1389/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1805 - val_loss: 0.3812\n",
      "Epoch 1390/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1787 - val_loss: 0.4167\n",
      "Epoch 1391/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1832 - val_loss: 0.4049\n",
      "Epoch 1392/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1996 - val_loss: 0.4036\n",
      "Epoch 1393/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1904 - val_loss: 0.3915\n",
      "Epoch 1394/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1695 - val_loss: 0.3995\n",
      "Epoch 1395/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1683 - val_loss: 0.3827\n",
      "Epoch 1396/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2005 - val_loss: 0.4174\n",
      "Epoch 1397/2000\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.2214 - val_loss: 0.4510\n",
      "Epoch 1398/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1931 - val_loss: 0.3855\n",
      "Epoch 1399/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2034 - val_loss: 0.4558\n",
      "Epoch 1400/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2409 - val_loss: 0.4068\n",
      "Epoch 1401/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1789 - val_loss: 0.4364\n",
      "Epoch 1402/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1729 - val_loss: 0.3938\n",
      "Epoch 1403/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1878 - val_loss: 0.4353\n",
      "Epoch 1404/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2041 - val_loss: 0.4254\n",
      "Epoch 1405/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1955 - val_loss: 0.4211\n",
      "Epoch 1406/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1729 - val_loss: 0.3931\n",
      "Epoch 1407/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2211 - val_loss: 0.4207\n",
      "Epoch 1408/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2238 - val_loss: 0.3886\n",
      "Epoch 1409/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2034 - val_loss: 0.4713\n",
      "Epoch 1410/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1893 - val_loss: 0.4085\n",
      "Epoch 1411/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1717 - val_loss: 0.3887\n",
      "Epoch 1412/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1734 - val_loss: 0.4008\n",
      "Epoch 1413/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1759 - val_loss: 0.4094\n",
      "Epoch 1414/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1839 - val_loss: 0.4160\n",
      "Epoch 1415/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1789 - val_loss: 0.4169\n",
      "Epoch 1416/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1740 - val_loss: 0.4135\n",
      "Epoch 1417/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1739 - val_loss: 0.3969\n",
      "Epoch 1418/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2036 - val_loss: 0.4329\n",
      "Epoch 1419/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1795 - val_loss: 0.4168\n",
      "Epoch 1420/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1749 - val_loss: 0.4075\n",
      "Epoch 1421/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1664 - val_loss: 0.4410\n",
      "Epoch 1422/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1934 - val_loss: 0.4043\n",
      "Epoch 1423/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1885 - val_loss: 0.4103\n",
      "Epoch 1424/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2050 - val_loss: 0.3985\n",
      "Epoch 1425/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1857 - val_loss: 0.4063\n",
      "Epoch 1426/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1655 - val_loss: 0.3997\n",
      "Epoch 1427/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1716 - val_loss: 0.4023\n",
      "Epoch 1428/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1678 - val_loss: 0.3957\n",
      "Epoch 1429/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1628 - val_loss: 0.4003\n",
      "Epoch 1430/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1745 - val_loss: 0.4290\n",
      "Epoch 1431/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1797 - val_loss: 0.4195\n",
      "Epoch 1432/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1770 - val_loss: 0.3983\n",
      "Epoch 1433/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1727 - val_loss: 0.4222\n",
      "Epoch 1434/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1991 - val_loss: 0.4016\n",
      "Epoch 1435/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1822 - val_loss: 0.3991\n",
      "Epoch 1436/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1870 - val_loss: 0.3960\n",
      "Epoch 1437/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1857 - val_loss: 0.4125\n",
      "Epoch 1438/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1739 - val_loss: 0.3952\n",
      "Epoch 1439/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1811 - val_loss: 0.4098\n",
      "Epoch 1440/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1806 - val_loss: 0.4026\n",
      "Epoch 1441/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1678 - val_loss: 0.3900\n",
      "Epoch 1442/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1798 - val_loss: 0.4034\n",
      "Epoch 1443/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2284 - val_loss: 0.4369\n",
      "Epoch 1444/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2014 - val_loss: 0.4035\n",
      "Epoch 1445/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1679 - val_loss: 0.3976\n",
      "Epoch 1446/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1683 - val_loss: 0.4142\n",
      "Epoch 1447/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1733 - val_loss: 0.3971\n",
      "Epoch 1448/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2273 - val_loss: 0.4009\n",
      "Epoch 1449/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1990 - val_loss: 0.3864\n",
      "Epoch 1450/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1972 - val_loss: 0.3907\n",
      "Epoch 1451/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1779 - val_loss: 0.4177\n",
      "Epoch 1452/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1899 - val_loss: 0.3971\n",
      "Epoch 1453/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1668 - val_loss: 0.3857\n",
      "Epoch 1454/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1596 - val_loss: 0.4114\n",
      "Epoch 1455/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1734 - val_loss: 0.4065\n",
      "Epoch 1456/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1686 - val_loss: 0.4385\n",
      "Epoch 1457/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1741 - val_loss: 0.3842\n",
      "Epoch 1458/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1818 - val_loss: 0.4457\n",
      "Epoch 1459/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2207 - val_loss: 0.4044\n",
      "Epoch 1460/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1733 - val_loss: 0.4049\n",
      "Epoch 1461/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1629 - val_loss: 0.3990\n",
      "Epoch 1462/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1862 - val_loss: 0.4319\n",
      "Epoch 1463/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2197 - val_loss: 0.4274\n",
      "Epoch 1464/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1834 - val_loss: 0.4040\n",
      "Epoch 1465/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1776 - val_loss: 0.3970\n",
      "Epoch 1466/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1691 - val_loss: 0.4031\n",
      "Epoch 1467/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2122 - val_loss: 0.4373\n",
      "Epoch 1468/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1861 - val_loss: 0.4123\n",
      "Epoch 1469/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1934 - val_loss: 0.4172\n",
      "Epoch 1470/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1791 - val_loss: 0.4348\n",
      "Epoch 1471/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1777 - val_loss: 0.3831\n",
      "Epoch 1472/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1813 - val_loss: 0.4016\n",
      "Epoch 1473/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1736 - val_loss: 0.4471\n",
      "Epoch 1474/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1999 - val_loss: 0.4059\n",
      "Epoch 1475/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2114 - val_loss: 0.4474\n",
      "Epoch 1476/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2040 - val_loss: 0.3959\n",
      "Epoch 1477/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1668 - val_loss: 0.3827\n",
      "Epoch 1478/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1628 - val_loss: 0.4044\n",
      "Epoch 1479/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1883 - val_loss: 0.4404\n",
      "Epoch 1480/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3230 - val_loss: 0.4362\n",
      "Epoch 1481/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2180 - val_loss: 0.4661\n",
      "Epoch 1482/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2161 - val_loss: 0.4255\n",
      "Epoch 1483/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1759 - val_loss: 0.3953\n",
      "Epoch 1484/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1576 - val_loss: 0.3835\n",
      "Epoch 1485/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1631 - val_loss: 0.3903\n",
      "Epoch 1486/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1791 - val_loss: 0.4136\n",
      "Epoch 1487/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1702 - val_loss: 0.4070\n",
      "Epoch 1488/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1656 - val_loss: 0.3972\n",
      "Epoch 1489/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1706 - val_loss: 0.4035\n",
      "Epoch 1490/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1744 - val_loss: 0.4100\n",
      "Epoch 1491/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1641 - val_loss: 0.4234\n",
      "Epoch 1492/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1815 - val_loss: 0.4029\n",
      "Epoch 1493/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1706 - val_loss: 0.4147\n",
      "Epoch 1494/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1801 - val_loss: 0.3939\n",
      "Epoch 1495/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.1556 - val_loss: 0.3878\n",
      "Epoch 1496/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1770 - val_loss: 0.4240\n",
      "Epoch 1497/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1798 - val_loss: 0.3924\n",
      "Epoch 1498/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1770 - val_loss: 0.4251\n",
      "Epoch 1499/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1749 - val_loss: 0.3822\n",
      "Epoch 1500/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1768 - val_loss: 0.4061\n",
      "Epoch 1501/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1701 - val_loss: 0.4050\n",
      "Epoch 1502/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2171 - val_loss: 0.4139\n",
      "Epoch 1503/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1696 - val_loss: 0.3712\n",
      "Epoch 1504/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1574 - val_loss: 0.3913\n",
      "Epoch 1505/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1603 - val_loss: 0.3859\n",
      "Epoch 1506/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1699 - val_loss: 0.4002\n",
      "Epoch 1507/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1711 - val_loss: 0.3863\n",
      "Epoch 1508/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1804 - val_loss: 0.4073\n",
      "Epoch 1509/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1874 - val_loss: 0.4885\n",
      "Epoch 1510/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2365 - val_loss: 0.4217\n",
      "Epoch 1511/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1831 - val_loss: 0.3899\n",
      "Epoch 1512/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1745 - val_loss: 0.3930\n",
      "Epoch 1513/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1820 - val_loss: 0.4515\n",
      "Epoch 1514/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1975 - val_loss: 0.4401\n",
      "Epoch 1515/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2219 - val_loss: 0.4283\n",
      "Epoch 1516/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1805 - val_loss: 0.4127\n",
      "Epoch 1517/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2018 - val_loss: 0.3955\n",
      "Epoch 1518/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1716 - val_loss: 0.4057\n",
      "Epoch 1519/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1792 - val_loss: 0.4033\n",
      "Epoch 1520/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1605 - val_loss: 0.3936\n",
      "Epoch 1521/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1671 - val_loss: 0.3988\n",
      "Epoch 1522/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1658 - val_loss: 0.4099\n",
      "Epoch 1523/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2197 - val_loss: 0.4405\n",
      "Epoch 1524/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1985 - val_loss: 0.4126\n",
      "Epoch 1525/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1942 - val_loss: 0.4321\n",
      "Epoch 1526/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1807 - val_loss: 0.4531\n",
      "Epoch 1527/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1830 - val_loss: 0.3981\n",
      "Epoch 1528/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1662 - val_loss: 0.4082\n",
      "Epoch 1529/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1623 - val_loss: 0.3836\n",
      "Epoch 1530/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1677 - val_loss: 0.4103\n",
      "Epoch 1531/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1995 - val_loss: 0.3969\n",
      "Epoch 1532/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1628 - val_loss: 0.4168\n",
      "Epoch 1533/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1684 - val_loss: 0.3912\n",
      "Epoch 1534/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1563 - val_loss: 0.3852\n",
      "Epoch 1535/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1630 - val_loss: 0.4017\n",
      "Epoch 1536/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1687 - val_loss: 0.3885\n",
      "Epoch 1537/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1699 - val_loss: 0.4529\n",
      "Epoch 1538/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1680 - val_loss: 0.4013\n",
      "Epoch 1539/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1842 - val_loss: 0.4070\n",
      "Epoch 1540/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2051 - val_loss: 0.4516\n",
      "Epoch 1541/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1943 - val_loss: 0.4072\n",
      "Epoch 1542/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1677 - val_loss: 0.4123\n",
      "Epoch 1543/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.1686 - val_loss: 0.4145\n",
      "Epoch 1544/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1827 - val_loss: 0.4104\n",
      "Epoch 1545/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1629 - val_loss: 0.4213\n",
      "Epoch 1546/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1767 - val_loss: 0.4002\n",
      "Epoch 1547/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1772 - val_loss: 0.4040\n",
      "Epoch 1548/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1814 - val_loss: 0.4041\n",
      "Epoch 1549/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.1851 - val_loss: 0.3924\n",
      "Epoch 1550/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1867 - val_loss: 0.4069\n",
      "Epoch 1551/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2031 - val_loss: 0.4247\n",
      "Epoch 1552/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1726 - val_loss: 0.3881\n",
      "Epoch 1553/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1618 - val_loss: 0.3926\n",
      "Epoch 1554/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1679 - val_loss: 0.3909\n",
      "Epoch 1555/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1570 - val_loss: 0.4196\n",
      "Epoch 1556/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2359 - val_loss: 0.4382\n",
      "Epoch 1557/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1892 - val_loss: 0.4096\n",
      "Epoch 1558/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1727 - val_loss: 0.3971\n",
      "Epoch 1559/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1599 - val_loss: 0.3958\n",
      "Epoch 1560/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1961 - val_loss: 0.4435\n",
      "Epoch 1561/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1732 - val_loss: 0.3765\n",
      "Epoch 1562/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1661 - val_loss: 0.3901\n",
      "Epoch 1563/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1674 - val_loss: 0.4019\n",
      "Epoch 1564/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1747 - val_loss: 0.4065\n",
      "Epoch 1565/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1566 - val_loss: 0.3967\n",
      "Epoch 1566/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1651 - val_loss: 0.4135\n",
      "Epoch 1567/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1772 - val_loss: 0.4106\n",
      "Epoch 1568/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1913 - val_loss: 0.4021\n",
      "Epoch 1569/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1797 - val_loss: 0.5068\n",
      "Epoch 1570/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1877 - val_loss: 0.3847\n",
      "Epoch 1571/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1591 - val_loss: 0.3913\n",
      "Epoch 1572/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1738 - val_loss: 0.4303\n",
      "Epoch 1573/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1992 - val_loss: 0.4158\n",
      "Epoch 1574/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2374 - val_loss: 0.4022\n",
      "Epoch 1575/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1878 - val_loss: 0.3990\n",
      "Epoch 1576/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1714 - val_loss: 0.4138\n",
      "Epoch 1577/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1632 - val_loss: 0.3889\n",
      "Epoch 1578/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1658 - val_loss: 0.3826\n",
      "Epoch 1579/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1566 - val_loss: 0.4009\n",
      "Epoch 1580/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1599 - val_loss: 0.4015\n",
      "Epoch 1581/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1588 - val_loss: 0.3950\n",
      "Epoch 1582/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1778 - val_loss: 0.4045\n",
      "Epoch 1583/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1888 - val_loss: 0.4117\n",
      "Epoch 1584/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1886 - val_loss: 0.4267\n",
      "Epoch 1585/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1912 - val_loss: 0.4026\n",
      "Epoch 1586/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1718 - val_loss: 0.4160\n",
      "Epoch 1587/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1622 - val_loss: 0.4030\n",
      "Epoch 1588/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1607 - val_loss: 0.3989\n",
      "Epoch 1589/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1862 - val_loss: 0.4130\n",
      "Epoch 1590/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1769 - val_loss: 0.4295\n",
      "Epoch 1591/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1754 - val_loss: 0.4071\n",
      "Epoch 1592/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1680 - val_loss: 0.4098\n",
      "Epoch 1593/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1789 - val_loss: 0.4031\n",
      "Epoch 1594/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1715 - val_loss: 0.4120\n",
      "Epoch 1595/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.1723 - val_loss: 0.4000\n",
      "Epoch 1596/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1666 - val_loss: 0.4281\n",
      "Epoch 1597/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1676 - val_loss: 0.3998\n",
      "Epoch 1598/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1613 - val_loss: 0.3932\n",
      "Epoch 1599/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1616 - val_loss: 0.4278\n",
      "Epoch 1600/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1706 - val_loss: 0.4008\n",
      "Epoch 1601/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1595 - val_loss: 0.3963\n",
      "Epoch 1602/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1879 - val_loss: 0.4321\n",
      "Epoch 1603/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2090 - val_loss: 0.3980\n",
      "Epoch 1604/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2046 - val_loss: 0.4168\n",
      "Epoch 1605/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1844 - val_loss: 0.3869\n",
      "Epoch 1606/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1667 - val_loss: 0.4157\n",
      "Epoch 1607/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1711 - val_loss: 0.4283\n",
      "Epoch 1608/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1633 - val_loss: 0.3894\n",
      "Epoch 1609/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1651 - val_loss: 0.4101\n",
      "Epoch 1610/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1640 - val_loss: 0.3998\n",
      "Epoch 1611/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1640 - val_loss: 0.3914\n",
      "Epoch 1612/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1612 - val_loss: 0.3904\n",
      "Epoch 1613/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2881 - val_loss: 0.4560\n",
      "Epoch 1614/2000\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.2481 - val_loss: 0.3845\n",
      "Epoch 1615/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1834 - val_loss: 0.4264\n",
      "Epoch 1616/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1906 - val_loss: 0.4463\n",
      "Epoch 1617/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1858 - val_loss: 0.4363\n",
      "Epoch 1618/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1692 - val_loss: 0.3948\n",
      "Epoch 1619/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1529 - val_loss: 0.3998\n",
      "Epoch 1620/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1469 - val_loss: 0.3864\n",
      "Epoch 1621/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1604 - val_loss: 0.4000\n",
      "Epoch 1622/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1508 - val_loss: 0.4058\n",
      "Epoch 1623/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1721 - val_loss: 0.4363\n",
      "Epoch 1624/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2288 - val_loss: 0.5451\n",
      "Epoch 1625/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2783 - val_loss: 0.4019\n",
      "Epoch 1626/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1748 - val_loss: 0.4070\n",
      "Epoch 1627/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1858 - val_loss: 0.3865\n",
      "Epoch 1628/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1714 - val_loss: 0.3980\n",
      "Epoch 1629/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1589 - val_loss: 0.3989\n",
      "Epoch 1630/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1597 - val_loss: 0.3922\n",
      "Epoch 1631/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1617 - val_loss: 0.4040\n",
      "Epoch 1632/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1591 - val_loss: 0.3843\n",
      "Epoch 1633/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1585 - val_loss: 0.4077\n",
      "Epoch 1634/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1644 - val_loss: 0.3984\n",
      "Epoch 1635/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1578 - val_loss: 0.3953\n",
      "Epoch 1636/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2003 - val_loss: 0.4119\n",
      "Epoch 1637/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1780 - val_loss: 0.4279\n",
      "Epoch 1638/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1736 - val_loss: 0.4088\n",
      "Epoch 1639/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1654 - val_loss: 0.3913\n",
      "Epoch 1640/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1628 - val_loss: 0.3919\n",
      "Epoch 1641/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1571 - val_loss: 0.4197\n",
      "Epoch 1642/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1681 - val_loss: 0.4469\n",
      "Epoch 1643/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1651 - val_loss: 0.4319\n",
      "Epoch 1644/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1956 - val_loss: 0.3981\n",
      "Epoch 1645/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1805 - val_loss: 0.4030\n",
      "Epoch 1646/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1689 - val_loss: 0.3960\n",
      "Epoch 1647/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1925 - val_loss: 0.4112\n",
      "Epoch 1648/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1747 - val_loss: 0.3940\n",
      "Epoch 1649/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1516 - val_loss: 0.3969\n",
      "Epoch 1650/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1612 - val_loss: 0.4156\n",
      "Epoch 1651/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1695 - val_loss: 0.4021\n",
      "Epoch 1652/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1795 - val_loss: 0.3820\n",
      "Epoch 1653/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1691 - val_loss: 0.4079\n",
      "Epoch 1654/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2103 - val_loss: 0.4189\n",
      "Epoch 1655/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1695 - val_loss: 0.3984\n",
      "Epoch 1656/2000\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.1639 - val_loss: 0.3848\n",
      "Epoch 1657/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1637 - val_loss: 0.4267\n",
      "Epoch 1658/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1725 - val_loss: 0.4040\n",
      "Epoch 1659/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1617 - val_loss: 0.3912\n",
      "Epoch 1660/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1778 - val_loss: 0.4321\n",
      "Epoch 1661/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2088 - val_loss: 0.4172\n",
      "Epoch 1662/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1850 - val_loss: 0.3874\n",
      "Epoch 1663/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1564 - val_loss: 0.3994\n",
      "Epoch 1664/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1656 - val_loss: 0.4050\n",
      "Epoch 1665/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1628 - val_loss: 0.4204\n",
      "Epoch 1666/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1657 - val_loss: 0.4214\n",
      "Epoch 1667/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1776 - val_loss: 0.4084\n",
      "Epoch 1668/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1610 - val_loss: 0.3964\n",
      "Epoch 1669/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1831 - val_loss: 0.3988\n",
      "Epoch 1670/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2436 - val_loss: 0.4250\n",
      "Epoch 1671/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.2526 - val_loss: 0.4259\n",
      "Epoch 1672/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1693 - val_loss: 0.3841\n",
      "Epoch 1673/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1553 - val_loss: 0.3965\n",
      "Epoch 1674/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1514 - val_loss: 0.4091\n",
      "Epoch 1675/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1608 - val_loss: 0.3883\n",
      "Epoch 1676/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1563 - val_loss: 0.4004\n",
      "Epoch 1677/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1647 - val_loss: 0.3862\n",
      "Epoch 1678/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1652 - val_loss: 0.4061\n",
      "Epoch 1679/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1722 - val_loss: 0.4140\n",
      "Epoch 1680/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1857 - val_loss: 0.3937\n",
      "Epoch 1681/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1626 - val_loss: 0.4062\n",
      "Epoch 1682/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1673 - val_loss: 0.4033\n",
      "Epoch 1683/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1713 - val_loss: 0.3935\n",
      "Epoch 1684/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1679 - val_loss: 0.4000\n",
      "Epoch 1685/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1924 - val_loss: 0.4010\n",
      "Epoch 1686/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1590 - val_loss: 0.4000\n",
      "Epoch 1687/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1641 - val_loss: 0.3924\n",
      "Epoch 1688/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1544 - val_loss: 0.3880\n",
      "Epoch 1689/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1547 - val_loss: 0.3944\n",
      "Epoch 1690/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1786 - val_loss: 0.4226\n",
      "Epoch 1691/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.1680 - val_loss: 0.3898\n",
      "Epoch 1692/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.1983 - val_loss: 0.4412\n",
      "Epoch 1693/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1728 - val_loss: 0.3994\n",
      "Epoch 1694/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1579 - val_loss: 0.3914\n",
      "Epoch 1695/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1529 - val_loss: 0.4036\n",
      "Epoch 1696/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1536 - val_loss: 0.3908\n",
      "Epoch 1697/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1504 - val_loss: 0.4076\n",
      "Epoch 1698/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1609 - val_loss: 0.3952\n",
      "Epoch 1699/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1570 - val_loss: 0.4037\n",
      "Epoch 1700/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1723 - val_loss: 0.3895\n",
      "Epoch 1701/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1817 - val_loss: 0.3902\n",
      "Epoch 1702/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2510 - val_loss: 0.5893\n",
      "Epoch 1703/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2976 - val_loss: 0.4606\n",
      "Epoch 1704/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2209 - val_loss: 0.3881\n",
      "Epoch 1705/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1807 - val_loss: 0.3785\n",
      "Epoch 1706/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1860 - val_loss: 0.3988\n",
      "Epoch 1707/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2274 - val_loss: 0.4071\n",
      "Epoch 1708/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1886 - val_loss: 0.3832\n",
      "Epoch 1709/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1727 - val_loss: 0.4008\n",
      "Epoch 1710/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1742 - val_loss: 0.3947\n",
      "Epoch 1711/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1523 - val_loss: 0.3854\n",
      "Epoch 1712/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1660 - val_loss: 0.3969\n",
      "Epoch 1713/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1641 - val_loss: 0.3990\n",
      "Epoch 1714/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1615 - val_loss: 0.4031\n",
      "Epoch 1715/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1494 - val_loss: 0.4014\n",
      "Epoch 1716/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1496 - val_loss: 0.3868\n",
      "Epoch 1717/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1466 - val_loss: 0.3957\n",
      "Epoch 1718/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1520 - val_loss: 0.3949\n",
      "Epoch 1719/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1597 - val_loss: 0.4279\n",
      "Epoch 1720/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1877 - val_loss: 0.4189\n",
      "Epoch 1721/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1830 - val_loss: 0.4292\n",
      "Epoch 1722/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1753 - val_loss: 0.4146\n",
      "Epoch 1723/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1688 - val_loss: 0.3961\n",
      "Epoch 1724/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1602 - val_loss: 0.3903\n",
      "Epoch 1725/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1579 - val_loss: 0.4075\n",
      "Epoch 1726/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1886 - val_loss: 0.4044\n",
      "Epoch 1727/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1955 - val_loss: 0.4080\n",
      "Epoch 1728/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1792 - val_loss: 0.4078\n",
      "Epoch 1729/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1957 - val_loss: 0.4207\n",
      "Epoch 1730/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1660 - val_loss: 0.4056\n",
      "Epoch 1731/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1643 - val_loss: 0.4032\n",
      "Epoch 1732/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1676 - val_loss: 0.4091\n",
      "Epoch 1733/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1789 - val_loss: 0.4029\n",
      "Epoch 1734/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1876 - val_loss: 0.3893\n",
      "Epoch 1735/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1559 - val_loss: 0.4028\n",
      "Epoch 1736/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1578 - val_loss: 0.3819\n",
      "Epoch 1737/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1649 - val_loss: 0.3923\n",
      "Epoch 1738/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1585 - val_loss: 0.4229\n",
      "Epoch 1739/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1722 - val_loss: 0.4212\n",
      "Epoch 1740/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1717 - val_loss: 0.4035\n",
      "Epoch 1741/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1720 - val_loss: 0.4053\n",
      "Epoch 1742/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1625 - val_loss: 0.4171\n",
      "Epoch 1743/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1711 - val_loss: 0.3970\n",
      "Epoch 1744/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1565 - val_loss: 0.4079\n",
      "Epoch 1745/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1639 - val_loss: 0.3980\n",
      "Epoch 1746/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1593 - val_loss: 0.4097\n",
      "Epoch 1747/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1627 - val_loss: 0.4071\n",
      "Epoch 1748/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1750 - val_loss: 0.4137\n",
      "Epoch 1749/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1728 - val_loss: 0.4349\n",
      "Epoch 1750/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1641 - val_loss: 0.4019\n",
      "Epoch 1751/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1526 - val_loss: 0.4023\n",
      "Epoch 1752/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1587 - val_loss: 0.4001\n",
      "Epoch 1753/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1597 - val_loss: 0.4089\n",
      "Epoch 1754/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1603 - val_loss: 0.4140\n",
      "Epoch 1755/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1691 - val_loss: 0.4135\n",
      "Epoch 1756/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1701 - val_loss: 0.4300\n",
      "Epoch 1757/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1686 - val_loss: 0.4389\n",
      "Epoch 1758/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1778 - val_loss: 0.5107\n",
      "Epoch 1759/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2049 - val_loss: 0.4292\n",
      "Epoch 1760/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1711 - val_loss: 0.4353\n",
      "Epoch 1761/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1589 - val_loss: 0.4070\n",
      "Epoch 1762/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1579 - val_loss: 0.4028\n",
      "Epoch 1763/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1514 - val_loss: 0.4007\n",
      "Epoch 1764/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1525 - val_loss: 0.4023\n",
      "Epoch 1765/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1509 - val_loss: 0.3906\n",
      "Epoch 1766/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1533 - val_loss: 0.3949\n",
      "Epoch 1767/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1688 - val_loss: 0.4021\n",
      "Epoch 1768/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1574 - val_loss: 0.4048\n",
      "Epoch 1769/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1638 - val_loss: 0.4058\n",
      "Epoch 1770/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1684 - val_loss: 0.4163\n",
      "Epoch 1771/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1802 - val_loss: 0.4209\n",
      "Epoch 1772/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1802 - val_loss: 0.4152\n",
      "Epoch 1773/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1549 - val_loss: 0.4112\n",
      "Epoch 1774/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1526 - val_loss: 0.3963\n",
      "Epoch 1775/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1540 - val_loss: 0.4009\n",
      "Epoch 1776/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1627 - val_loss: 0.3957\n",
      "Epoch 1777/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1581 - val_loss: 0.3975\n",
      "Epoch 1778/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1766 - val_loss: 0.4096\n",
      "Epoch 1779/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1762 - val_loss: 0.3911\n",
      "Epoch 1780/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1581 - val_loss: 0.4061\n",
      "Epoch 1781/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1803 - val_loss: 0.3843\n",
      "Epoch 1782/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1616 - val_loss: 0.4314\n",
      "Epoch 1783/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1769 - val_loss: 0.4590\n",
      "Epoch 1784/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1737 - val_loss: 0.4084\n",
      "Epoch 1785/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1553 - val_loss: 0.3961\n",
      "Epoch 1786/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1512 - val_loss: 0.4176\n",
      "Epoch 1787/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1559 - val_loss: 0.3947\n",
      "Epoch 1788/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1513 - val_loss: 0.3884\n",
      "Epoch 1789/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1834 - val_loss: 0.4060\n",
      "Epoch 1790/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1715 - val_loss: 0.4080\n",
      "Epoch 1791/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1691 - val_loss: 0.4472\n",
      "Epoch 1792/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2343 - val_loss: 0.4493\n",
      "Epoch 1793/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1945 - val_loss: 0.4063\n",
      "Epoch 1794/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1634 - val_loss: 0.3969\n",
      "Epoch 1795/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1544 - val_loss: 0.3892\n",
      "Epoch 1796/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1730 - val_loss: 0.4284\n",
      "Epoch 1797/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1988 - val_loss: 0.3961\n",
      "Epoch 1798/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1591 - val_loss: 0.3916\n",
      "Epoch 1799/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1463 - val_loss: 0.4083\n",
      "Epoch 1800/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1485 - val_loss: 0.3905\n",
      "Epoch 1801/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1489 - val_loss: 0.4212\n",
      "Epoch 1802/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1520 - val_loss: 0.4069\n",
      "Epoch 1803/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1555 - val_loss: 0.4072\n",
      "Epoch 1804/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1571 - val_loss: 0.4084\n",
      "Epoch 1805/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1649 - val_loss: 0.3886\n",
      "Epoch 1806/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1537 - val_loss: 0.3912\n",
      "Epoch 1807/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1543 - val_loss: 0.4254\n",
      "Epoch 1808/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1519 - val_loss: 0.3995\n",
      "Epoch 1809/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1564 - val_loss: 0.3972\n",
      "Epoch 1810/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1567 - val_loss: 0.3847\n",
      "Epoch 1811/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1671 - val_loss: 0.3942\n",
      "Epoch 1812/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1621 - val_loss: 0.3939\n",
      "Epoch 1813/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1515 - val_loss: 0.4041\n",
      "Epoch 1814/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1561 - val_loss: 0.4072\n",
      "Epoch 1815/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1579 - val_loss: 0.3995\n",
      "Epoch 1816/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1747 - val_loss: 0.4304\n",
      "Epoch 1817/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1768 - val_loss: 0.4106\n",
      "Epoch 1818/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1688 - val_loss: 0.4367\n",
      "Epoch 1819/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1639 - val_loss: 0.4015\n",
      "Epoch 1820/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1470 - val_loss: 0.3934\n",
      "Epoch 1821/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1793 - val_loss: 0.4476\n",
      "Epoch 1822/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1961 - val_loss: 0.4516\n",
      "Epoch 1823/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2275 - val_loss: 0.4205\n",
      "Epoch 1824/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1728 - val_loss: 0.3911\n",
      "Epoch 1825/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1583 - val_loss: 0.3976\n",
      "Epoch 1826/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1583 - val_loss: 0.4003\n",
      "Epoch 1827/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1565 - val_loss: 0.4116\n",
      "Epoch 1828/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1624 - val_loss: 0.3918\n",
      "Epoch 1829/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1492 - val_loss: 0.4022\n",
      "Epoch 1830/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1516 - val_loss: 0.4017\n",
      "Epoch 1831/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1489 - val_loss: 0.3992\n",
      "Epoch 1832/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1558 - val_loss: 0.4165\n",
      "Epoch 1833/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1829 - val_loss: 0.4491\n",
      "Epoch 1834/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1662 - val_loss: 0.4066\n",
      "Epoch 1835/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1521 - val_loss: 0.4106\n",
      "Epoch 1836/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1531 - val_loss: 0.4090\n",
      "Epoch 1837/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1526 - val_loss: 0.4163\n",
      "Epoch 1838/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1703 - val_loss: 0.4384\n",
      "Epoch 1839/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1621 - val_loss: 0.4327\n",
      "Epoch 1840/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1641 - val_loss: 0.4159\n",
      "Epoch 1841/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1882 - val_loss: 0.5642\n",
      "Epoch 1842/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2841 - val_loss: 0.4402\n",
      "Epoch 1843/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2141 - val_loss: 0.4139\n",
      "Epoch 1844/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1675 - val_loss: 0.3891\n",
      "Epoch 1845/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1561 - val_loss: 0.4150\n",
      "Epoch 1846/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1543 - val_loss: 0.3892\n",
      "Epoch 1847/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1629 - val_loss: 0.4036\n",
      "Epoch 1848/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1596 - val_loss: 0.4024\n",
      "Epoch 1849/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1619 - val_loss: 0.4091\n",
      "Epoch 1850/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1588 - val_loss: 0.3991\n",
      "Epoch 1851/2000\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.1447 - val_loss: 0.3929\n",
      "Epoch 1852/2000\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.1487 - val_loss: 0.4145\n",
      "Epoch 1853/2000\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.1548 - val_loss: 0.3861\n",
      "Epoch 1854/2000\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.1473 - val_loss: 0.4037\n",
      "Epoch 1855/2000\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 0.1531 - val_loss: 0.3987\n",
      "Epoch 1856/2000\n",
      "80/80 [==============================] - 2s 19ms/step - loss: 0.1680 - val_loss: 0.4519\n",
      "Epoch 1857/2000\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 0.3146 - val_loss: 0.4794\n",
      "Epoch 1858/2000\n",
      "80/80 [==============================] - 2s 21ms/step - loss: 0.2426 - val_loss: 0.4501\n",
      "Epoch 1859/2000\n",
      "80/80 [==============================] - 1s 19ms/step - loss: 0.1796 - val_loss: 0.3849\n",
      "Epoch 1860/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1668 - val_loss: 0.4166\n",
      "Epoch 1861/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1526 - val_loss: 0.3948\n",
      "Epoch 1862/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1443 - val_loss: 0.3856\n",
      "Epoch 1863/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1472 - val_loss: 0.3831\n",
      "Epoch 1864/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1485 - val_loss: 0.3851\n",
      "Epoch 1865/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1427 - val_loss: 0.4025\n",
      "Epoch 1866/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1523 - val_loss: 0.3810\n",
      "Epoch 1867/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1564 - val_loss: 0.4171\n",
      "Epoch 1868/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1775 - val_loss: 0.3993\n",
      "Epoch 1869/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1699 - val_loss: 0.4087\n",
      "Epoch 1870/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1498 - val_loss: 0.3936\n",
      "Epoch 1871/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1572 - val_loss: 0.4191\n",
      "Epoch 1872/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1482 - val_loss: 0.4009\n",
      "Epoch 1873/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1462 - val_loss: 0.3881\n",
      "Epoch 1874/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1500 - val_loss: 0.4095\n",
      "Epoch 1875/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1647 - val_loss: 0.4347\n",
      "Epoch 1876/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1866 - val_loss: 0.4192\n",
      "Epoch 1877/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1722 - val_loss: 0.4221\n",
      "Epoch 1878/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1783 - val_loss: 0.3873\n",
      "Epoch 1879/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1638 - val_loss: 0.4356\n",
      "Epoch 1880/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2873 - val_loss: 0.4401\n",
      "Epoch 1881/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.2158 - val_loss: 0.4143\n",
      "Epoch 1882/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1592 - val_loss: 0.3893\n",
      "Epoch 1883/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1492 - val_loss: 0.3960\n",
      "Epoch 1884/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1504 - val_loss: 0.3803\n",
      "Epoch 1885/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1504 - val_loss: 0.3979\n",
      "Epoch 1886/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1538 - val_loss: 0.3903\n",
      "Epoch 1887/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1710 - val_loss: 0.4362\n",
      "Epoch 1888/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1623 - val_loss: 0.4074\n",
      "Epoch 1889/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1709 - val_loss: 0.4217\n",
      "Epoch 1890/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1797 - val_loss: 0.3913\n",
      "Epoch 1891/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1631 - val_loss: 0.3909\n",
      "Epoch 1892/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1550 - val_loss: 0.4045\n",
      "Epoch 1893/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1473 - val_loss: 0.3903\n",
      "Epoch 1894/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1514 - val_loss: 0.4011\n",
      "Epoch 1895/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1477 - val_loss: 0.4331\n",
      "Epoch 1896/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.2160 - val_loss: 0.3935\n",
      "Epoch 1897/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1591 - val_loss: 0.4047\n",
      "Epoch 1898/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1543 - val_loss: 0.3972\n",
      "Epoch 1899/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1635 - val_loss: 0.4059\n",
      "Epoch 1900/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1643 - val_loss: 0.4101\n",
      "Epoch 1901/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1507 - val_loss: 0.3967\n",
      "Epoch 1902/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1474 - val_loss: 0.3889\n",
      "Epoch 1903/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1456 - val_loss: 0.4015\n",
      "Epoch 1904/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1446 - val_loss: 0.3908\n",
      "Epoch 1905/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1490 - val_loss: 0.4103\n",
      "Epoch 1906/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1523 - val_loss: 0.4336\n",
      "Epoch 1907/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1978 - val_loss: 0.4495\n",
      "Epoch 1908/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1708 - val_loss: 0.3933\n",
      "Epoch 1909/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1606 - val_loss: 0.4075\n",
      "Epoch 1910/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1583 - val_loss: 0.3897\n",
      "Epoch 1911/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1635 - val_loss: 0.5492\n",
      "Epoch 1912/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2368 - val_loss: 0.3994\n",
      "Epoch 1913/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1892 - val_loss: 0.3929\n",
      "Epoch 1914/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1481 - val_loss: 0.3885\n",
      "Epoch 1915/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1466 - val_loss: 0.3851\n",
      "Epoch 1916/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1529 - val_loss: 0.4150\n",
      "Epoch 1917/2000\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.1686 - val_loss: 0.4017\n",
      "Epoch 1918/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1594 - val_loss: 0.4033\n",
      "Epoch 1919/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1510 - val_loss: 0.3976\n",
      "Epoch 1920/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1536 - val_loss: 0.3977\n",
      "Epoch 1921/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1473 - val_loss: 0.3839\n",
      "Epoch 1922/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1501 - val_loss: 0.3895\n",
      "Epoch 1923/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1584 - val_loss: 0.4131\n",
      "Epoch 1924/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2263 - val_loss: 0.4259\n",
      "Epoch 1925/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1987 - val_loss: 0.3936\n",
      "Epoch 1926/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1595 - val_loss: 0.3934\n",
      "Epoch 1927/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1708 - val_loss: 0.4230\n",
      "Epoch 1928/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1460 - val_loss: 0.3967\n",
      "Epoch 1929/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1437 - val_loss: 0.3930\n",
      "Epoch 1930/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1589 - val_loss: 0.4241\n",
      "Epoch 1931/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1481 - val_loss: 0.4087\n",
      "Epoch 1932/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1628 - val_loss: 0.3806\n",
      "Epoch 1933/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1446 - val_loss: 0.4127\n",
      "Epoch 1934/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1459 - val_loss: 0.3749\n",
      "Epoch 1935/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1666 - val_loss: 0.4344\n",
      "Epoch 1936/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1911 - val_loss: 0.4182\n",
      "Epoch 1937/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1592 - val_loss: 0.3909\n",
      "Epoch 1938/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1449 - val_loss: 0.3771\n",
      "Epoch 1939/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1477 - val_loss: 0.4169\n",
      "Epoch 1940/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1516 - val_loss: 0.4071\n",
      "Epoch 1941/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1766 - val_loss: 0.4260\n",
      "Epoch 1942/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1999 - val_loss: 0.3915\n",
      "Epoch 1943/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1597 - val_loss: 0.4231\n",
      "Epoch 1944/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1478 - val_loss: 0.3757\n",
      "Epoch 1945/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1966 - val_loss: 0.4879\n",
      "Epoch 1946/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1963 - val_loss: 0.4026\n",
      "Epoch 1947/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1532 - val_loss: 0.3809\n",
      "Epoch 1948/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1593 - val_loss: 0.3844\n",
      "Epoch 1949/2000\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.1463 - val_loss: 0.3960\n",
      "Epoch 1950/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1459 - val_loss: 0.3924\n",
      "Epoch 1951/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1431 - val_loss: 0.4044\n",
      "Epoch 1952/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1549 - val_loss: 0.3989\n",
      "Epoch 1953/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1496 - val_loss: 0.3918\n",
      "Epoch 1954/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1579 - val_loss: 0.4287\n",
      "Epoch 1955/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2214 - val_loss: 0.4328\n",
      "Epoch 1956/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1816 - val_loss: 0.4001\n",
      "Epoch 1957/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1553 - val_loss: 0.4253\n",
      "Epoch 1958/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1480 - val_loss: 0.3992\n",
      "Epoch 1959/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1478 - val_loss: 0.3817\n",
      "Epoch 1960/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1527 - val_loss: 0.3896\n",
      "Epoch 1961/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1452 - val_loss: 0.3933\n",
      "Epoch 1962/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1502 - val_loss: 0.3979\n",
      "Epoch 1963/2000\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.1719 - val_loss: 0.4402\n",
      "Epoch 1964/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1676 - val_loss: 0.4039\n",
      "Epoch 1965/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1535 - val_loss: 0.3869\n",
      "Epoch 1966/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1391 - val_loss: 0.3895\n",
      "Epoch 1967/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1507 - val_loss: 0.3886\n",
      "Epoch 1968/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1516 - val_loss: 0.3865\n",
      "Epoch 1969/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1439 - val_loss: 0.4046\n",
      "Epoch 1970/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1565 - val_loss: 0.3983\n",
      "Epoch 1971/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1442 - val_loss: 0.4053\n",
      "Epoch 1972/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1607 - val_loss: 0.4152\n",
      "Epoch 1973/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2071 - val_loss: 0.4210\n",
      "Epoch 1974/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1791 - val_loss: 0.3720\n",
      "Epoch 1975/2000\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1556 - val_loss: 0.4159\n",
      "Epoch 1976/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1520 - val_loss: 0.3961\n",
      "Epoch 1977/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1590 - val_loss: 0.4235\n",
      "Epoch 1978/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1564 - val_loss: 0.4043\n",
      "Epoch 1979/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1503 - val_loss: 0.3843\n",
      "Epoch 1980/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1449 - val_loss: 0.3906\n",
      "Epoch 1981/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1569 - val_loss: 0.4149\n",
      "Epoch 1982/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1526 - val_loss: 0.3877\n",
      "Epoch 1983/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1458 - val_loss: 0.4061\n",
      "Epoch 1984/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1670 - val_loss: 0.4114\n",
      "Epoch 1985/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1612 - val_loss: 0.4180\n",
      "Epoch 1986/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1684 - val_loss: 0.4055\n",
      "Epoch 1987/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1697 - val_loss: 0.3909\n",
      "Epoch 1988/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1574 - val_loss: 0.4075\n",
      "Epoch 1989/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1647 - val_loss: 0.4688\n",
      "Epoch 1990/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1561 - val_loss: 0.3812\n",
      "Epoch 1991/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1471 - val_loss: 0.4042\n",
      "Epoch 1992/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1457 - val_loss: 0.3893\n",
      "Epoch 1993/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1401 - val_loss: 0.3902\n",
      "Epoch 1994/2000\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.1399 - val_loss: 0.3971\n",
      "Epoch 1995/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1513 - val_loss: 0.3850\n",
      "Epoch 1996/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2126 - val_loss: 0.4295\n",
      "Epoch 1997/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1739 - val_loss: 0.3898\n",
      "Epoch 1998/2000\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1748 - val_loss: 0.3952\n",
      "Epoch 1999/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1733 - val_loss: 0.3972\n",
      "Epoch 2000/2000\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1453 - val_loss: 0.3888\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = tf.keras.losses.MeanSquaredError(), optimizer = opt)\n",
    "history = model.fit(inputs, outputs, epochs = 2000, batch_size = 100, verbose = 1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "minimal-manual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10.0, 10.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0, 257, 257)\n",
    "for k in range(1):\n",
    "    \n",
    "    i = random.randint(0,len(rdfs))\n",
    "    test = np.array([rdfs[i]])\n",
    "\n",
    "    modelpred = model.predict([test])\n",
    "    plt.plot(x, pots[i] - 30, label = 'Target')\n",
    "    plt.plot(x, modelpred[0] - 30, label = 'Predict')\n",
    "    \n",
    "plt.legend()\n",
    "plt.ylim(-10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fatal-safety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model6/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Model6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-regular",
   "metadata": {},
   "source": [
    "model1: loss ~ 1.5 \\\n",
    "        val_loss ~ 0.9\n",
    "        \n",
    "        \n",
    "model2: ~0.17 \\\n",
    "        ~0.47\n",
    "        \n",
    "        \n",
    "model3: ~0.17 \\\n",
    "        ~0.45\n",
    "        \n",
    "        \n",
    "model4: loss: 0.1957 \\\n",
    "val_loss: 0.3878\n",
    "\n",
    "model5: loss: 0.2075 \\\n",
    "val_loss: 0.3788\n",
    "\n",
    "Model6: loss: 0.1921 - val_loss: 0.3682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "funny-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inp = np.array(input_rdf.DFs[0].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "statistical-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pot = mt.ReadPot('NaNa.pot', quiet= True)\n",
    "inpot = np.array(input_pot.DFs[0].y)\n",
    "z = np.linspace(0, len(inpot), len(inpot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "continent-drunk",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d402631df490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_inp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodelpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "test = np.array([test_inp])\n",
    "\n",
    "modelpred = model.predict([test])\n",
    "\n",
    "z = np.linspace(0, 15, 300)\n",
    "\n",
    "pred = np.zeros(300)\n",
    "act = np.zeros(300)\n",
    "for i in range(1, 301):\n",
    "    if i < len(modelpred[0]):\n",
    "        pred[-i] = modelpred[0][-i]\n",
    "    else:\n",
    "        pred[-i] = 50000 + 3*i\n",
    "        \n",
    "        \n",
    "    if i < len(inpot):\n",
    "        act[-i] = inpot[-i]\n",
    "    else:\n",
    "        act[-i] = 50000 + 3*i\n",
    "plt.plot(z, pred - 30, label = 'Predict')\n",
    "plt.plot(z, act, label = 'True')\n",
    "    \n",
    "plt.legend()\n",
    "plt.ylim(-10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vocal-recruitment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 257)               66306     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 80)                20640     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 257)               20817     \n",
      "=================================================================\n",
      "Total params: 114,243\n",
      "Trainable params: 114,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loadmodel = tf.keras.models.load_model('Model6', compile = False)\n",
    "loadmodel.summary()\n",
    "loadmodel.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "smaller-bermuda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = np.array([test_inp])\n",
    "\n",
    "modelpred = loadmodel.predict([test])\n",
    "\n",
    "z = np.linspace(0, 15, 300)\n",
    "\n",
    "pred = np.zeros(300)\n",
    "act = np.zeros(300)\n",
    "for i in range(1, 301):\n",
    "    if i < len(modelpred[0]):\n",
    "        pred[-i] = modelpred[0][-i]\n",
    "    else:\n",
    "        pred[-i] = 50000 + 500*i\n",
    "        \n",
    "        \n",
    "    if i < len(inpot):\n",
    "        act[-i] = inpot[-i]\n",
    "    else:\n",
    "        act[-i] = 50000 + 50*i\n",
    "plt.plot(z, pred - 30, label = 'Predict')\n",
    "plt.plot(z, act, label = 'True')\n",
    "    \n",
    "plt.legend()\n",
    "plt.ylim(-5,12)\n",
    "plt.xlabel('r $\\AA$')\n",
    "plt.ylabel('Potential Kcal/mol')\n",
    "\n",
    "plt.title('Pots')\n",
    "plt.savefig('potentials.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "graphic-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(input_rdf.AtomTypes)\n",
    "N_NB = int(N*(N+1)/2) #number of non-bonded interactions\n",
    "N_B = int(0) #number of bonded interactions\n",
    "N_A = int(0) #number of angle interactions\n",
    "\n",
    "AtomList = []\n",
    "for i in range(len(input_rdf.AtomTypes)):\n",
    "    AtomList.append(input_rdf.AtomTypes[i])\n",
    "        \n",
    "AtomPairs = list(it.combinations_with_replacement(AtomList, r = 2)) #all possible atoms pairs, no repeats\n",
    "    \n",
    "general_section = \" &General \\n\" + \" NTypes= {} \\n\".format(N) + \" N_NB= {} \\n\".format(N_NB) + \\\n",
    "\" N_B= {} \\n\".format(N_B) + \" N_A= {} \\n\".format(N_A)+ \" Min= {} \\n\".format(Min) + \" Max= {} \\n\".format(Max) + \\\n",
    "\" NPoints= {} \\n\".format(NPoints) + \" &EndGeneral \\n\"\n",
    "    \n",
    "potential_section = ''\n",
    "for i in range(len(AtomPairs)):\n",
    "    single_pot = pred\n",
    "    header_section = \" &Potential \\n\" + \" Name= {}-{} \\n\".format(AtomPairs[i][0], AtomPairs[i][1]) + \\\n",
    "    \" Type = NB \\n\" \\\n",
    "    + \" Min= {} \\n\".format(Min) + \" Max= {} \\n\".format(Max) + \" NPoints= {} \\n\".format(NPoints) + \\\n",
    "    \" AtomTypes= {},{} \\n\".format(AtomPairs[i][0], AtomPairs[i][1])\n",
    "        \n",
    "    table_section = ' &Table \\n'\n",
    "    for j in range(len(rvalues)):\n",
    "        rval = format(rvalues[j], '.7f')\n",
    "        potval = format(single_pot[j], '.7f')\n",
    "        table_section = table_section + \"     \" + str(rval) + \"  \" + str(potval) + \" \\n\"\n",
    "            \n",
    "    table_section = table_section + \" &EndTable \\n\" + \" &EndPotential \\n\"\n",
    "        \n",
    "    potential_section = potential_section + header_section + table_section\n",
    "        \n",
    "potfile = open('Test/pred.pot', 'w')\n",
    "potfile.write(general_section)\n",
    "potfile.write(potential_section)\n",
    "potfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "infrared-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "pot = mt.ReadPot('Test/pred.pot', quiet = True)\n",
    "mt.PotsExport(pot, MDEngine = 'LAMMPS', Rmaxtable = 15.5, PHImaxtable = 180.0, \\\n",
    "                npoints = 2500, Umax = 6000.0, \\\n",
    "                interpol = True, method = 'gauss', sigma = 0.5, noplot=True, hardcopy = False, \\\n",
    "                filename = 'Test/predpotential')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "choice-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "testrdf = mt.ReadRDF('Test/pred.rdf', quiet = True)\n",
    "truerdf = mt.ReadRDF('Setup/NaNa.rdf', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adopted-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rvalues\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        obj = testrdf.DFs[0]\n",
    "    if i == 1:\n",
    "        obj = truerdf.DFs[0]\n",
    "    N = 1\n",
    "    for j in range(N):\n",
    "        holder = np.empty((300))\n",
    "        startindex = np.where(x == round(obj.x[0], 3))[0][0]\n",
    "        for k in range(300):\n",
    "            if k < startindex:\n",
    "                holder[k] = 0\n",
    "            elif k >= startindex and k-startindex < len(obj.x):\n",
    "                holder[k] = obj.y[k-startindex]\n",
    "            else:\n",
    "                holder[k] = 0\n",
    "        \n",
    "    if i == 0:\n",
    "        test = holder\n",
    "        \n",
    "    if i == 1:\n",
    "        true = holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "latter-perception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, test, label = 'Pred')\n",
    "plt.plot(x, true, label = 'True')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('r $\\AA$')\n",
    "plt.ylabel('RDF')\n",
    "plt.title('RDFs')\n",
    "\n",
    "plt.savefig('rdfs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "coordinate-durham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(924.05176, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test = np.array([test_inp])\n",
    "\n",
    "modelpred = loadmodel.predict([test])\n",
    "\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "print(loss_object(test[0], modelpred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sublime-somerset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257 244\n"
     ]
    }
   ],
   "source": [
    "print(len(test[0]), len(inpot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "expected-independence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0037\n",
      "39.417767\n"
     ]
    }
   ],
   "source": [
    "print(test[0][0])\n",
    "print(modelpred[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "swedish-jacob",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.8450441360473633, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "ytrue = np.zeros(244)\n",
    "ypred = np.zeros(244)\n",
    "\n",
    "for i in range(1, 245):\n",
    "    ytrue[-i] = inpot[-i]\n",
    "    ypred[-i] = test[0][-i]\n",
    "    \n",
    "print(loss_object(ytrue, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-primary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
